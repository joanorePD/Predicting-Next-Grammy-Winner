# Can access the values of a matrix by using the row/column names
d["batman",]
## OPERATIONS BETWEEN MATRIXES
a = matrix(0:5, nrow=2)
a
b = matrix(seq(0,10,2), nrow=2)
b
dim(b)
a + b
a*b
a/b
a
b
a*b
a&*%b
a%*%b
t(a)
# Can transpose using t()
t(a)
A = t(a)%*%b
A
A = t(a)%*%b
A
Solve(A)
solve(A)
solve(b)
solbe(B)
sole(B)
B=t(b)%*%a
solve(B)
solve(B)%*%B
smoker = c("yes", "tes", "no", "no", "yes")
survey = data.frame(weight, height, smoker)
survey
smoker = c("yes", "yes", "no", "no", "yes")
survey = data.frame(weight, height, smoker)
survey
cbind(weight, height, smoker)
view(survey)
# Data frames are also displayed differently in the Data Environment Pane
view(survey)
survey{1, 1]
survey[1, 1]
# Can access values much like a vector
survey[1:3, c("height", "smoker")]
survey
# Can view columns by using the $ operator
survey$height
survey$height
# Objects in R have attributes
# all objects have a basic attribute that is called a class of object
# can use the function class() to see the type of object
class(survey)
# There are lots of "is." functions to check if an object is a certain type
is.data.frame(survey)
# [1] TRUE
is.matrix(a)
# [1] TRUE
is.integer(a)
# [1] TRUE
is.data.frame(a)
# a list is a very flexible object
# it is a vector, but every entry of a list may be an object of any class
Lst = list("Fred", 3, c(4, 7, 9), M, Survey)
# a list is a very flexible object
# it is a vector, but every entry of a list may be an object of any class
Lst = list("Fred", 3, c(4, 7, 9), A, Survey)
# a list is a very flexible object
# it is a vector, but every entry of a list may be an object of any class
Lst = list("Fred", 3, c(4, 7, 9), A, survey)
Lst
# Can also give names to entries of the list
Lst = list(name="Fred", n.child=3, child.age=c(4, 7, 9), my.matrix=A, my.data=survey)
Lst
Lst$my.data
x <- 302
n <- 407
p.hat <- x/n
p.hat
se.hat <- sqrt(p.hat*(1-p.hat)/n)
se.hat
ic.upper <- p.hat + 1.96*se.hat
ic.lower <- p.hat - 1.96*se.hat
round(c(ic.lower, ic.upper), 3)
# R-function for exact CI
binom.test(x, n, conf.level=0.95)
# R-function for approximated CI
prop.test(x, n, conf.level=0.95)
# R-function for exact CI
binom.test(x, n, conf.level=0.95)
# R-function for approximated CI
prop.test(x, n, conf.level=0.95)
# R-function for exact CI
binom.test(x, n, conf.level=0.95)
# R-function for approximated CI
prop.test(x, n, conf.level=0.95)
x <- 302
n <- 407
p.hat <- x/n
p.hat
se.hat <- sqrt(p.hat*(1-p.hat)/n)
se.hat
ic.upper <- p.hat + 1.96*se.hat
ic.lower <- p.hat - 1.96*se.hat
round(c(ic.lower, ic.upper), 3)
# R-function for exact CI
# - performs exact inference
# He suggests you use this function to produce exact results
binom.test(x, n, conf.level=0.95)
# R-function for approximated CI
prop.test(x, n, conf.level=0.95)
n <- 40
x <- 11
p.hat <- x/n
p.0 <- 0.25
# IMPORTANT: note that the standard
# error under the null hypothesis is
# computed using p.0 rather than p.hat
#
se.0 <- sqrt(p.0*(1-p.0)/n)
z <- (p.hat-p.0)/se.0
p.value <- 1-pnorm(z)
p.value
# R-function for exact test
binom.test(x, n, p=0.25, alternative="greater")
z
# R-function for exact test
binom.test(x, n, p=0.25, alternative="greater")
# R-function for approximated CI
prop.test(x, n, p=0.25, alternative="greater")
# data
x <- c(56,65,17,7,16,22,3,4,2,3,8,4,3,30,4,43)
# density of Weibull distribution
dW <- function(x, alpha, beta) beta*alpha^-1*(x/alpha)^(beta-1)*exp(-(x/alpha)^beta)
dW(1, alpha=1, beta=1)
dweibull(1, shape=1, scale=1)
# log-likelihood function
# Note that we need to give the parameters a single vector
loglik.W <- function(par, data){#par=c(alpha, beta)
alpha <- par[1]
beta  <- par[2]
l <- sum(log(dW(data, alpha, beta)))
return(l)
}
# minus log-likelihood function
minus.loglik.W <- function(par, data) -loglik.W(par, data)
# Maximum likelihood estimates of alpha and beta
opt.result <- nlminb(c(1,1), minus.loglik.W, data=x)
opt.result$par
alpha.hat <- opt.result$par[1]
beta.hat  <- opt.result$par[2]
alpha.hat
beta.hat
$iterations
library(numDeriv)
H <- hessian(minus.loglik.W, c(alpha.hat, beta.hat), data=x)
j <- -H
H
J
j
H <- hessian(minus.loglik.W, c(alpha.hat, beta.hat), data=x)
j
asy.var <- solve(H)
asy.var
# uses MINUS loglik here
H <- hessian(minus.loglik.W, c(alpha.hat, beta.hat), data=x)
asy.var <- solve(H)
asy.var
se.alpha <- sqrt(asy.var[1,1])
se.beta  <- sqrt(asy.var[2,2])
alpha.hat+c(-1,1)*qnorm(0.975)*se.alpha
beta.hat +c(-1,1)*qnorm(0.975)*se.beta
alpha.hat+c(-1,1)*qnorm(0.975)*se.alpha
beta.hat +c(-1,1)*qnorm(0.975)*se.beta
ls
is()
ls()
x = c(1, 2 NA, 4)
c <- c(1, 2, NA, 4)
x <- c(1, 2, NA, 4)
y <- c(1, NA, 3, 4)
x + y
Advertising <- read.csv("Advertising.csv")
Advertising <- read.csv("Advertising.csv")
Advertising <- read.csv("Advertising.csv")
library(ISLR2)
data("Default")
attach(Default)
l <- default=="Yes"
plot(balance, income, col=l+1, pch=l*15+1)
mod.out <- glm(default ~ income + balance, family = binomial)
logistic.prob <- predict(mod.out, type="response")
logistic.pred <- rep("No", 10000)
logistic.pred[logistic.prob>0.5] <- "Yes"
default[200:215]
logistic.pred[200:215]
default[200:215]==logistic.pred[200:215]
summary(mod.out)
beta <- coefficients(mod.out)
intercept <- -beta[1]/beta[2]
slope <- -beta[3]/beta[2]
plot(balance, income, col=l+1, pch=l*15+1)
abline(intercept, slope, lwd=2, col="blue")
table(logistic.pred, default)
# overall (training) error rate
(225+38)/10000
trivial.pred <- rep("No", 10000)
table(trivial.pred, default)
# overall (training) error rate for the trivial classifier
333/10000
# (training) error rate for defaulting-customers
table(logistic.pred, default)
225/(225+108)
logistic.pred <- rep("No", 10000)
logistic.pred[logistic.prob>0.2] <- "Yes"
table(logistic.pred, default)
# overall (training) error rate
(133+271)/10000
# (training) error rate among individuals who defaulted
133/(133+200)
logistic.pred <- rep("No", 10000)
logistic.pred[logistic.prob>0.5] <- "Yes"
CM <- table(logistic.pred, default)
# rearrange rows and columns to match
# the confusion matrix on the slides
CM <- CM[2:1, 2:1]
logistic.pred <- rep("No", 10000)
logistic.pred[logistic.prob>0.5] <- "Yes"
CM <- table(logistic.pred, default)
CM
# rearrange rows and columns to match
# the confusion matrix on the slides
CM <- CM[2:1, 2:1]
CM
# add margins to the confusion matrix
# obtain the totals
CM <- addmargins(CM, margin = c(1, 2))
CM
# false positive rate = FP / N (false pos over total negative)
fpr <- 38/9667
fpr
fpr <- CM["Yes", "No"]/CM["Sum", "No"]
fpr
specif <- 1 - fpr
specif
tpr <- 108/333
tpr
tpr <- CM["Yes", "Yes"]/CM["Sum", "Yes"]
tpr
CM
library(pROC)
# default are true predictions, then prob are your predictions
roc.out <- roc(default, logistic.prob)
roc.out <- roc(default, logistic.prob, levels=c("No", "Yes"))
# argument "legacy.axes" is a logical indicating if the specificity axis
# (x axis) must be plotted as as decreasing “specificity” (FALSE, the default)
# or increasing “1 - specificity” (TRUE)
plot(roc.out)
# ROC curve traditionally represented with respect to specificity
#   and Sensitivity
#   Specificity is what you want large, so the x-axis is backwards
#      (decreases from 1)
plot(roc.out, legacy.axes=TRUE)
plot(roc.out,  print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")
# and you can compute AUC
auc(roc.out)
plot(roc.out,  print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")
# specificity and sensitivity for a given threshold
coords(roc.out, x=0.2)
# threshold that maximizes the sum of sensitivity and specificity
coords(roc.out, x="best")
library(ISLR2)
names(Smarket)
dim(Smarket)
summary(Smarket)
# summary prints the variables
pairs(Smarket)
cor(Smarket)
cor(Smarket)
round(cor(Smarket[,-9]), 3)
attach(Smarket)
plot(Volume, xlab="time")
glm.fits <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=Smarket, family=binomial)
summary(glm.fits)
# alternative syntax using the "dot"
glm.fits <- glm(Direction~.-Today-Year, data=Smarket, family=binomial)
summary(glm.fits)
# obtain fitted probabilities
glm.probs <- predict(glm.fits, type="response")
glm.probs[1:10]
# check the coding of Direction (to properly interpret probabilities)
contrasts(Direction)
glm.pred <- rep("Down",1250)
glm.pred[glm.probs>.5] <- "Up"
table(glm.pred,Direction)
# overall (training) error rate
(141+457)/1250
# 0.4784; or 47%
#   We know that random guessing (toss a coin to choose investments), error rate is 50%
#      This is slightly smaller than 50%, but this is a training error rate
#      This error rate is underestimated, true error is most likely large (probably 50%)
Direction
# proportions of "Up" and "Down"
table(Direction)/length(Direction)
library(pROC)
roc.out <- roc(Direction, glm.probs, levels=c("Down", "Up"))
plot(roc.out, print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")
# reduced set of predictors
####################################
# Let's try tor educe number of predictiors: Lag1 and Lag2
glm.fits <- glm(Direction~Lag1+Lag2,data=Smarket,family=binomial)
glm.probs <- predict(glm.fits,type="response")
glm.pred <- rep("Down",1250)
glm.pred[glm.probs>.5]="Up"
table(glm.pred,Direction)
# overall error rate
(102+488)/1250
table(Direction)/length(Direction)
train <- (Year<2005)
# creates a logical vector, T for data before 2005 and F for later
# Train on data from years less than 2005
Smarket.2005 <- Smarket[!train,]
# When we take S market and !train logical vector, we obtain the data for 2005
#   This is out validation set
dim(Smarket.2005)
# Could also
Direction.2005 <- Direction[!train]
# fit for same variables on Smarket on the subset train
#   Only works with the data where Train vector index is True
glm.fits <- glm(Direction~.-Today-Year, family=binomial, data=Smarket, subset=train)
# prediction on hold-out set
glm.probs <- predict(glm.fits,Smarket.2005,type="response")
glm.pred <- rep("Down",252)
glm.pred[glm.probs>.5] <- "Up"
table(glm.pred,Direction.2005)
(97+34)/252
table(Direction.2005)/length(Direction.2005)
x <- 1:4
x
x[2]
x[0]
x[0]
setwd("~/CMK/College/Padova/Spring 2023/Statistical Learning Mod B/Final Project/SL_Project")
load("final_df_n_str.RData")
Sys.setenv(LANG = "en")
library(pROC)
library(MASS)
library(ROSE)
library(confintr)
library(ggplot2)
library(correlation)
library(corrplot)
library(class)
library(caret)
library(glmnet)
data = final_df_n_str
data = data[,c("track_name", "artist_name", "IsWinner", "Year","year", "followers", "acousticness", "danceability", "duration_ms",
"energy", "instrumentalness", "key", "liveness", "loudness", "mode",
"tempo", "time_signature", "valence")]
data$Year[data$Year == "Undefined"] <- data$year[data$Year == "Undefined"]
data = data[,c("track_name","artist_name", "IsWinner", "Year", "followers", "acousticness", "danceability", "duration_ms",
"energy", "instrumentalness", "key", "liveness", "loudness", "mode",
"tempo", "time_signature", "valence")]
data$track_name == "Closing Time"
data$track_name == "Smells Like Teen Spirit"
data$track_name == "Don't Wanna Fight"
data[914, ]
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
load("final_df_n_str.RData")
Sys.setenv(LANG = "en")
library(pROC)
library(MASS)
library(ROSE)
library(confintr)
library(ggplot2)
library(correlation)
library(corrplot)
library(class)
library(caret)
library(glmnet)
data = final_df_n_str
data = data[,c("track_name", "artist_name", "IsWinner", "Year","year", "followers", "acousticness", "danceability", "duration_ms",
"energy", "instrumentalness", "key", "liveness", "loudness", "mode",
"tempo", "time_signature", "valence")]
data$Year[data$Year == "Undefined"] <- data$year[data$Year == "Undefined"]
data = data[,c("track_name","artist_name", "IsWinner", "Year", "followers", "acousticness", "danceability", "duration_ms",
"energy", "instrumentalness", "key", "liveness", "loudness", "mode",
"tempo", "time_signature", "valence")]
data$track_name == "Closing Time"
data$track_name == "Smells Like Teen Spirit"
data$track_name == "Don't Wanna Fight"
data[914, ]
data[789,]
data[669,]
data = data[-c(669, 789, 914),]
sum(data$Year < 1992)
nrow(data)
data = data[!data$Year < 1992,]
names = paste0(data$track_name, " - ", data$artist_name)
data = data[,c("IsWinner", "Year", "followers", "acousticness", "danceability", "duration_ms",
"energy", "instrumentalness", "key", "liveness", "loudness", "mode",
"tempo", "time_signature", "valence")]
data = cbind(names = names, data)
data$IsWinner[data$IsWinner == "Winner"] = 1
data$IsWinner[data$IsWinner == "Nominee"] = 1
data$IsWinner[data$IsWinner == "Nothing"] = 0
data$IsWinner[data$IsWinner == "Nothing"] = 0
data$IsWinner = as.integer(data$IsWinner)
data$Year = as.integer(data$Year)
data$mode = as.factor(data$mode)
data$key = as.factor(data$key)
data$time_signature = as.factor(data$time_signature)
summary(data)
summary(data$IsWinner)
training_size = floor(0.8 * nrow(data))
set.seed(42)
train_ind = sample(seq_len(nrow(data)), size = training_size)
training_set = data[train_ind,]
test_set = data[-train_ind,]
summary(training_set)
sum(data$IsWinner == 1)/ sum(data$IsWinner == 0)
sum(training_set$IsWinner == 1)/ sum(training_set$IsWinner == 0)
summary(data$IsWinner)
summary(data)
load("final_df_n_str.RData")
Sys.setenv(LANG = "en")
library(pROC)
library(MASS)
library(ROSE)
library(confintr)
library(ggplot2)
library(correlation)
library(corrplot)
library(class)
library(caret)
library(glmnet)
data = final_df_n_str
data = final_df_n_str
data = data[,c("track_name", "artist_name", "IsWinner", "Year","year", "followers", "acousticness", "danceability", "duration_ms",
"energy", "instrumentalness", "key", "liveness", "loudness", "mode",
"tempo", "time_signature", "valence")]
data$Year[data$Year == "Undefined"] <- data$year[data$Year == "Undefined"]
data = data[,c("track_name","artist_name", "IsWinner", "Year", "followers", "acousticness", "danceability", "duration_ms",
"energy", "instrumentalness", "key", "liveness", "loudness", "mode",
"tempo", "time_signature", "valence")]
data$track_name == "Closing Time"
data$track_name == "Smells Like Teen Spirit"
data$track_name == "Don't Wanna Fight"
data[914, ]
data[789,]
data[669,]
data = data[-c(669, 789, 914),]
sum(data$Year < 1992)
nrow(data)
data = data[!data$Year < 1992,]
load("~/CMK/College/Padova/Spring 2023/Statistical Learning Mod B/Final Project/SL_Project/final_df_n_str.RData")
data = final_df_n_str
data = data[,c("track_name", "artist_name", "IsWinner", "Year","year", "followers", "acousticness", "danceability", "duration_ms",
"energy", "instrumentalness", "key", "liveness", "loudness", "mode",
"tempo", "time_signature", "valence")]
data$Year[data$Year == "Undefined"] <- data$year[data$Year == "Undefined"]
data = data[,c("track_name","artist_name", "IsWinner", "Year", "followers", "acousticness", "danceability", "duration_ms",
"energy", "instrumentalness", "key", "liveness", "loudness", "mode",
"tempo", "time_signature", "valence")]
data$track_name == "Closing Time"
data$track_name == "Smells Like Teen Spirit"
data$track_name == "Don't Wanna Fight"
data[914, ]
data[789,]
data[669,]
data = data[-c(669, 789, 914),]
sum(data$Year < 1992)
nrow(data)
data = data[!data$Year < 1992,]
names = paste0(data$track_name, " - ", data$artist_name)
data = data[,c("IsWinner", "Year", "followers", "acousticness", "danceability", "duration_ms",
"energy", "instrumentalness", "key", "liveness", "loudness", "mode",
"tempo", "time_signature", "valence")]
data = cbind(names = names, data)
data$IsWinner[data$IsWinner == "Winner"] = 1
data$IsWinner[data$IsWinner == "Nominee"] = 1
data$IsWinner[data$IsWinner == "Nothing"] = 0
data$IsWinner = as.integer(data$IsWinner)
data$Year = as.integer(data$Year)
data$mode = as.factor(data$mode)
data$key = as.factor(data$key)
data$time_signature = as.factor(data$time_signature)
summary(data)
summary(data$IsWinner)
training_size = floor(0.8 * nrow(data))
set.seed(42)
train_ind = sample(seq_len(nrow(data)), size = training_size)
training_set = data[train_ind,]
test_set = data[-train_ind,]
summary(training_set)
sum(data$IsWinner == 1)/ sum(data$IsWinner == 0)
sum(training_set$IsWinner == 1)/ sum(training_set$IsWinner == 0)
attach(training_set)
# Correlations between continuous variables
cor_matrix = cor(training_set[,c(-1, -2, -10, -13, -15)])
corrplot(cor_matrix)
pairs(training_set[,c(-1, -2, -10, -13, -15)], lower.panel = panel.smooth)
# Correlations between continuous variables
cor_matrix = cor(training_set[,c(-1, -2, -10, -13, -15)])
# Correlations between continuous variables
cor_matrix = cor(training_set[,c(-1, -2, -10, -13, -15)])
corrplot(cor_matrix)
