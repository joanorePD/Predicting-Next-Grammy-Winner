tem_time.aov <- aov(tempo ~ time_signature)
summary(tem_time.aov) # SIGNIFICANT
val_time.aov <- aov(valence ~ time_signature)
summary(val_time.aov) # SIGNIFICANT
# Partial correlations
correlation(training_set[,c(-1, -2, -10, -13, -15)], partial = TRUE)
# Plots of variables with the largest partial correlation
ggplot(data = training_set, aes(danceability, valence)) + geom_jitter(color = "blue")
ggplot(data = training_set, aes(loudness, energy)) + geom_jitter(color = "blue")
ggplot(data = training_set, aes(acousticness, energy)) + geom_jitter(color = "blue")
#Weird song veeeeeeeeeeeeeeeeeeeeeeeery long
which.max(data$duration_ms)
data[504, ]
##############
# Checking distributions
par(mfrow= c(2, 5))
# Continuous variables
hist(followers)
hist(acousticness)
hist(danceability)
hist(duration_ms)
hist(energy)
hist(instrumentalness)
hist(liveness)
hist(loudness)
hist(tempo)
hist(valence)
# Categorical variables
par(mfrow = c(1, 3))
barplot(table(key), main = "Key distribution")
barplot(table(mode), main = "Mode")
barplot(table(time_signature), main = "Time signature")
# Relationships between dependent and independent variables
par(mfrow= c(2, 5))
boxplot(danceability ~ training_set$IsWinner)
boxplot(followers ~ training_set$IsWinner)
boxplot(acousticness ~ training_set$IsWinner)
boxplot(duration_ms ~ training_set$IsWinner)
boxplot(energy ~ training_set$IsWinner)
boxplot(instrumentalness ~ training_set$IsWinner)
boxplot(liveness ~ training_set$IsWinner)
boxplot(loudness ~ training_set$IsWinner)
boxplot(tempo ~ training_set$IsWinner)
boxplot(valence ~ training_set$IsWinner)
par(mfrow = c(1, 1))
chisq.test(key, training_set$IsWinner)
chisq.test(mode, training_set$IsWinner)
chisq.test(time_signature, training_set$IsWinner)
cramersv(matrix(c(as.numeric(key), as.numeric(training_set$IsWinner)), ncol = 2))
cramersv(matrix(c(as.numeric(mode), as.numeric(training_set$IsWinner)), ncol = 2))
cramersv(matrix(c(as.numeric(time_signature), as.numeric(training_set$IsWinner)), ncol = 2))
table(mode, training_set$IsWinner)
table(time_signature, training_set$IsWinner)
###############################################################################
### Model fitting
## Oversampling
oversampled_train_data = ovun.sample(IsWinner ~., data = training_set[,-1], method = "over", p = 0.5, seed = 42)$data
# Checking oversampled training set balance
sum(oversampled_train_data$IsWinner == 0)
sum(oversampled_train_data$IsWinner == 1)
## Simple logistic model
logistic = glm(IsWinner ~ ., data = training_set[,c(-1,-2)], family = "binomial")
summary(logistic)
# Stepwise variable selection
log_back = stepAIC(logistic, direction = "backward")
log_for = stepAIC(logistic, direction = "forward")
log_both =  stepAIC(logistic, direction = "both")
# Fitting the reduced model
logistic_reduced = glm(IsWinner ~ danceability + loudness + followers + valence + duration_ms + acousticness, data = training_set,  family = "binomial")
summary(logistic_reduced)
# Compiuting predictions
logistic_predictions = predict(logistic_reduced, newdata = test_set[,c(-1, -2)], type = "response")
# Threshold = 0.2
logistic_predictions_02 = ifelse(logistic_predictions > 0.2, 1, 0)
logistic_accuracy_02 = sum(logistic_predictions_02 == test_set[2]) / dim(test_set[2])[1]
table(test_set$IsWinner, logistic_predictions_02)
false_positive_logistic_02 = table(test_set$IsWinner, logistic_predictions_02)[3]
negative_logistic_02 = table(test_set$IsWinner, logistic_predictions_02)[1] + table(test_set$IsWinner, logistic_predictions_02)[2]
typeIerror_logistic_02 = false_positive_logistic_02 / negative_logistic_02
true_positive_logistic_02 = table(test_set$IsWinner, logistic_predictions_02)[4]
positive_logistic_02 = table(test_set$IsWinner, logistic_predictions_02)[2] + table(test_set$IsWinner, logistic_predictions_02)[4]
sensitivity_logistic_02 = true_positive_logistic_02 / positive_logistic_02
# Threshold = 0.3
logistic_predictions_03 = ifelse(logistic_predictions > 0.3, 1, 0)
logistic_accuracy_03 = sum(logistic_predictions_03 == test_set[2]) / dim(test_set[2])[1]
table(test_set$IsWinner, logistic_predictions_03)
false_positive_logistic_03 = table(test_set$IsWinner, logistic_predictions_03)[3]
negative_logistic_03 = table(test_set$IsWinner, logistic_predictions_03)[1] + table(test_set$IsWinner, logistic_predictions_03)[2]
typeIerror_logistic_03 = false_positive_logistic_03 / negative_logistic_03
true_positive_logistic_03 = table(test_set$IsWinner, logistic_predictions_03)[4]
positive_logistic_03 = table(test_set$IsWinner, logistic_predictions_03)[2] + table(test_set$IsWinner, logistic_predictions_03)[4]
sensitivity_logistic_03 = true_positive_logistic_03 / positive_logistic_03
# Threshold = 0.4
logistic_predictions_04 = ifelse(logistic_predictions > 0.4, 1, 0)
logistic_accuracy_04 = sum(logistic_predictions_04 == test_set[2]) / dim(test_set[2])[1]
table(test_set$IsWinner, logistic_predictions_04)
false_positive_logistic_04 = table(test_set$IsWinner, logistic_predictions_04)[3]
negative_logistic_04 = table(test_set$IsWinner, logistic_predictions_04)[1] + table(test_set$IsWinner, logistic_predictions_04)[2]
typeIerror_logistic_04 = false_positive_logistic_04 / negative_logistic_04
true_positive_logistic_04 = table(test_set$IsWinner, logistic_predictions_04)[4]
positive_logistic_04 = table(test_set$IsWinner, logistic_predictions_04)[2] + table(test_set$IsWinner, logistic_predictions_04)[4]
sensitivity_logistic_04 = true_positive_logistic_04 / positive_logistic_04
# ROC curve
roc.out <- roc(test_set$IsWinner, logistic_predictions)
plot(roc.out, print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")
auc(roc.out)
# Fitting logistic oversampled
logistic_over = glm(as.numeric(unlist(oversampled_train_data[1])) ~ ., data = oversampled_train_data[-1], family = "binomial")
summary(logistic_over)
log_over_back = stepAIC(logistic_over, direction = "backward")
log_over_for = stepAIC(logistic_over, direction = "forward")
log_over_both =  stepAIC(logistic_over, direction = "both")
response_variable_over = as.numeric(unlist(oversampled_train_data[1]))
reduced_variables_over = as.matrix(oversampled_train_data[,c(2, 3, 4, 6, 8, 9, 11, 14, 15)], ncol = 9)
reduced_train_data_over = matrix(c(
response_variable_over,
as.numeric(reduced_variables_over[,1]),
as.numeric(reduced_variables_over[,2]),
as.numeric(reduced_variables_over[,3]),
as.numeric(reduced_variables_over[,4]),
as.numeric(reduced_variables_over[,5]),
as.factor(reduced_variables_over[,6]),
as.numeric(reduced_variables_over[,7]),
as.factor(reduced_variables_over[,8]),
as.numeric(reduced_variables_over[,9])
), ncol = 10)
colnames(reduced_train_data_over) = c("IsWinner", "Year", "followers", "acousticness", "duration_ms",
"instrumentalness", "key", "loudness", "time_signature", "valence" )
logistic_reduced_over = glm(response_variable_over ~ Year + followers + acousticness
+ duration_ms + instrumentalness + key + loudness +
+ time_signature + valence, data = oversampled_train_data,
family = "binomial")
logistic_over_predictions = predict(logistic_reduced_over, newdata = test_set[,c(-1, -2)], type = "response")
# Threshold = 0.2
logistic_over_predictions_02 = ifelse(logistic_over_predictions > 0.2, 1, 0)
logistic_over_accuracy_02 = sum(logistic_over_predictions_02 == test_set[2]) / dim(test_set[2])[1]
table(test_set$IsWinner, logistic_over_predictions_02)
false_positive_logistic_over_02 = table(test_set$IsWinner, logistic_over_predictions_02)[3]
negative_logistic_over_02 = table(test_set$IsWinner, logistic_over_predictions_02)[1] + table(test_set$IsWinner, logistic_over_predictions_02)[2]
typeIerror_logistic_over_02 = false_positive_logistic_over_02 / negative_logistic_over_02
true_positive_logistic_over_02 = table(test_set$IsWinner, logistic_over_predictions_02)[4]
positive_logistic_over_02 = table(test_set$IsWinner, logistic_over_predictions_02)[2] + table(test_set$IsWinner, logistic_over_predictions_02)[4]
sensitivity_logistic_over_02 = true_positive_logistic_over_02 / positive_logistic_over_02
# Threshold = 0.3
logistic_over_predictions_03 = ifelse(logistic_over_predictions > 0.3, 1, 0)
logistic_over_accuracy_03 = sum(logistic_over_predictions_03 == test_set[2]) / dim(test_set[2])[1]
table(test_set$IsWinner, logistic_over_predictions_03)
false_positive_logistic_over_03 = table(test_set$IsWinner, logistic_over_predictions_03)[3]
negative_logistic_over_03 = table(test_set$IsWinner, logistic_over_predictions_03)[1] + table(test_set$IsWinner, logistic_over_predictions_03)[2]
typeIerror_logistic_over_03 = false_positive_logistic_over_03 / negative_logistic_over_03
true_positive_logistic_over_03 = table(test_set$IsWinner, logistic_over_predictions_03)[4]
positive_logistic_over_03 = table(test_set$IsWinner, logistic_over_predictions_03)[2] + table(test_set$IsWinner, logistic_over_predictions_03)[4]
sensitivity_logistic_over_03 = true_positive_logistic_over_03 / positive_logistic_over_03
# Threshold = 0.4
logistic_over_predictions_04 = ifelse(logistic_over_predictions > 0.4, 1, 0)
logistic_over_accuracy_04 = sum(logistic_over_predictions_04 == test_set[2]) / dim(test_set[2])[1]
table(test_set$IsWinner, logistic_over_predictions_04)
false_positive_logistic_over_04 = table(test_set$IsWinner, logistic_over_predictions_04)[3]
negative_logistic_over_04 = table(test_set$IsWinner, logistic_over_predictions_04)[1] + table(test_set$IsWinner, logistic_over_predictions_04)[2]
typeIerror_logistic_over_04 = false_positive_logistic_over_04 / negative_logistic_over_04
true_positive_logistic_over_04 = table(test_set$IsWinner, logistic_over_predictions_04)[4]
positive_logistic_over_04 = table(test_set$IsWinner, logistic_over_predictions_04)[2] + table(test_set$IsWinner, logistic_over_predictions_04)[4]
sensitivity_logistic_over_04 = true_positive_logistic_over_04 / positive_logistic_over_04
# ROC curve
roc.out <- roc(test_set$IsWinner, logistic_over_predictions)
plot(roc.out, print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")
auc(roc.out)
# Linear discriminant analysis
# Checking normality of the predictors conditioned to the classes of the variable IsWinner
shapiro.test(danceability[IsWinner == 0]) # Yes
shapiro.test(danceability[IsWinner == 1]) # Yes
shapiro.test(followers[IsWinner == 0]) # No
shapiro.test(followers[IsWinner == 1]) # No
shapiro.test(acousticness[IsWinner == 0]) # No
shapiro.test(acousticness[IsWinner == 1]) # No
shapiro.test(duration_ms[IsWinner == 0]) # No
shapiro.test(duration_ms[IsWinner == 1]) # No
shapiro.test(energy[IsWinner == 0]) # No
shapiro.test(energy[IsWinner == 1]) # No
shapiro.test(instrumentalness[IsWinner == 0]) # No
shapiro.test(instrumentalness[IsWinner == 1]) # No
shapiro.test(liveness[IsWinner == 0]) # No
shapiro.test(liveness[IsWinner == 1]) # No
shapiro.test(loudness[IsWinner == 0]) # No
shapiro.test(loudness[IsWinner == 1]) # No
shapiro.test(tempo[IsWinner == 0]) # No
shapiro.test(tempo[IsWinner == 1]) # No
shapiro.test(valence[IsWinner == 0]) # No
shapiro.test(valence[IsWinner == 1]) # No
par(mfrow = c(1, 2))
# danceability looking normal
qqnorm(danceability[IsWinner == 0])
grid()
qqline(danceability[IsWinner == 0],lwd = 2, col = "red")
qqnorm(danceability[IsWinner == 1])
grid()
qqline(danceability[IsWinner == 1],lwd = 2, col = "red")
# followers huge right tail
qqnorm(followers[IsWinner == 0])
grid()
qqline(followers[IsWinner == 0],lwd = 2, col = "red")
qqnorm(followers[IsWinner == 1])
grid()
qqline(followers[IsWinner == 1],lwd = 2, col = "red")
# acousticness S shaped
qqnorm(acousticness[IsWinner == 0])
grid()
qqline(acousticness[IsWinner == 0],lwd = 2, col = "red")
qqnorm(acousticness[IsWinner == 1])
grid()
qqline(acousticness[IsWinner == 1],lwd = 2, col = "red")
# duration_ms  big right tail
qqnorm(duration_ms[IsWinner == 0])
grid()
qqline(duration_ms[IsWinner == 0],lwd = 2, col = "red")
qqnorm(duration_ms[IsWinner == 1])
grid()
qqline(duration_ms[IsWinner == 1],lwd = 2, col = "red")
# energy tails not normal
qqnorm(energy[IsWinner == 0])
grid()
qqline(energy[IsWinner == 0],lwd = 2, col = "red")
qqnorm(energy[IsWinner == 1])
grid()
qqline(energy[IsWinner == 1],lwd = 2, col = "red")
# instrumentalness huge right tail
qqnorm(instrumentalness[IsWinner == 0])
grid()
qqline(instrumentalness[IsWinner == 0],lwd = 2, col = "red")
qqnorm(instrumentalness[IsWinner == 1])
grid()
qqline(instrumentalness[IsWinner == 1],lwd = 2, col = "red")
# liveness S shaped
qqnorm(liveness[IsWinner == 0])
grid()
qqline(liveness[IsWinner == 0],lwd = 2, col = "red")
qqnorm(liveness[IsWinner == 1])
grid()
qqline(liveness[IsWinner == 1],lwd = 2, col = "red")
# loudness tails not normal
qqnorm(loudness[IsWinner == 0])
grid()
qqline(loudness[IsWinner == 0],lwd = 2, col = "red")
qqnorm(loudness[IsWinner == 1])
grid()
qqline(loudness[IsWinner == 1],lwd = 2, col = "red")
# tempo tails slightly not normal
qqnorm(tempo[IsWinner == 0])
grid()
qqline(tempo[IsWinner == 0],lwd = 2, col = "red")
qqnorm(tempo[IsWinner == 1])
grid()
qqline(tempo[IsWinner == 1],lwd = 2, col = "red")
# valence tails slightly not normal
qqnorm(valence[IsWinner == 0])
grid()
qqline(valence[IsWinner == 0],lwd = 2, col = "red")
qqnorm(valence[IsWinner == 1])
grid()
qqline(valence[IsWinner == 1],lwd = 2, col = "red")
# Applying transformations to the predictors in the attempt of making them normal
# followers plots improved, one passes the test
par(mfrow = c(1, 1))
b_followers <- boxcox(lm(followers ~ 1))
lambda <- b_followers$x[which.max(b_followers$y)]
followers_tran <- (followers ^ lambda - 1) / lambda
par(mfrow = c(1, 2))
qqnorm(followers_tran[IsWinner == 0])
grid()
qqline(followers_tran[IsWinner == 0],lwd = 2, col = "red")
qqnorm(followers_tran[IsWinner == 1])
grid()
qqline(followers_tran[IsWinner == 1],lwd = 2, col = "red")
shapiro.test(followers_tran[IsWinner == 0]) # No
shapiro.test(followers_tran[IsWinner == 1]) # Yes
# Acousticness plots improved, test not passed
par(mfrow = c(1, 1))
b_acousticness <- boxcox(lm(acousticness ~ 1))
lambda <- b_acousticness$x[which.max(b_acousticness$y)]
acousticness_tran <- (acousticness ^ lambda - 1) / lambda
par(mfrow = c(1, 2))
qqnorm(acousticness_tran[IsWinner == 0])
grid()
qqline(acousticness_tran[IsWinner == 0],lwd = 2, col = "red")
qqnorm(acousticness_tran[IsWinner == 1])
grid()
qqline(acousticness_tran[IsWinner == 1],lwd = 2, col = "red")
shapiro.test(acousticness_tran[IsWinner == 0]) # No
shapiro.test(acousticness_tran[IsWinner == 1]) # No
# duration_ms plots improved, test not passed
par(mfrow = c(1, 1))
b_duration_ms <- boxcox(lm(duration_ms ~ 1))
lambda <- b_duration_ms$x[which.max(b_duration_ms$y)]
duration_ms_tran <- (duration_ms ^ lambda - 1) / lambda
par(mfrow = c(1, 2))
qqnorm(duration_ms_tran[IsWinner == 0])
grid()
qqline(duration_ms_tran[IsWinner == 0],lwd = 2, col = "red")
qqnorm(duration_ms_tran[IsWinner == 1])
grid()
qqline(duration_ms_tran[IsWinner == 1],lwd = 2, col = "red")
shapiro.test(duration_ms_tran[IsWinner == 0]) # No
shapiro.test(duration_ms_tran[IsWinner == 1]) # No
# energy plots are pretty much the same test not passed
par(mfrow = c(1, 1))
b_energy <- boxcox(lm(energy ~ 1))
lambda <- b_energy$x[which.max(b_energy$y)]
energy_tran <- (energy ^ lambda - 1) / lambda
par(mfrow = c(1, 2))
qqnorm(energy_tran[IsWinner == 0])
grid()
qqline(energy_tran[IsWinner == 0],lwd = 2, col = "red")
qqnorm(energy_tran[IsWinner == 1])
grid()
qqline(energy_tran[IsWinner == 1],lwd = 2, col = "red")
shapiro.test(energy_tran[IsWinner == 0]) # No
shapiro.test(energy_tran[IsWinner == 1]) # No
# instrumentalness can't apply the boxcox transformation because there are some 0's
# I tried to apply a linear transformation before but the plot is weird
par(mfrow = c(1, 1))
new_instrumentalness = instrumentalness + 1e-05
b_instrumentalness <- boxcox(lm(new_instrumentalness ~ 1))
lambda <- b_instrumentalness$x[which.max(b_instrumentalness$y)]
instrumentalness_tran <- (new_instrumentalness ^ lambda - 1) / lambda
par(mfrow = c(1, 2))
qqnorm(instrumentalness_tran[IsWinner == 0])
grid()
qqline(instrumentalness_tran[IsWinner == 0],lwd = 2, col = "red")
qqnorm(instrumentalness_tran[IsWinner == 1])
grid()
qqline(instrumentalness_tran[IsWinner == 1],lwd = 2, col = "red")
shapiro.test(instrumentalness_tran[IsWinner == 0]) # No
shapiro.test(instrumentalness_tran[IsWinner == 1]) # No
# liveness plots improved a lot, test not passed because of a few points in the tails
par(mfrow = c(1, 1))
b_liveness <- boxcox(lm(liveness ~ 1))
lambda <- b_liveness$x[which.max(b_liveness$y)]
liveness_tran <- (liveness ^ lambda - 1) / lambda
par(mfrow = c(1, 2))
qqnorm(liveness_tran[IsWinner == 0])
grid()
qqline(liveness_tran[IsWinner == 0],lwd = 2, col = "red")
qqnorm(liveness_tran[IsWinner == 1])
grid()
qqline(liveness_tran[IsWinner == 1],lwd = 2, col = "red")
shapiro.test(liveness_tran[IsWinner == 0]) # No
shapiro.test(liveness_tran[IsWinner == 1]) # No
# loudness boxcox transformation not directly applicable because the variable
# is always negative, I multiplied the values by -1 and then applied it,
# it gave very good results, but we need to pay attention to the interpretation
par(mfrow = c(1, 1))
new_loudness = loudness * (-1)
b_loudness <- boxcox(lm(new_loudness ~ 1))
lambda <- b_loudness$x[which.max(b_loudness$y)]
loudness_tran <- (new_loudness ^ lambda - 1) / lambda
par(mfrow = c(1, 2))
qqnorm(loudness_tran[IsWinner == 0])
grid()
qqline(loudness_tran[IsWinner == 0],lwd = 2, col = "red")
qqnorm(loudness_tran[IsWinner == 1])
grid()
qqline(loudness_tran[IsWinner == 1],lwd = 2, col = "red")
shapiro.test(loudness_tran[IsWinner == 0]) # Yes
shapiro.test(loudness_tran[IsWinner == 1]) # Yes
# tempo, slight improvement in the plots
par(mfrow = c(1, 1))
b_tempo <- boxcox(lm(tempo ~ 1))
lambda <- b_tempo$x[which.max(b_tempo$y)]
tempo_tran <- (tempo ^ lambda - 1) / lambda
par(mfrow = c(1, 2))
qqnorm(tempo_tran[IsWinner == 0])
grid()
qqline(tempo_tran[IsWinner == 0],lwd = 2, col = "red")
qqnorm(tempo_tran[IsWinner == 1])
grid()
qqline(tempo_tran[IsWinner == 1],lwd = 2, col = "red")
shapiro.test(tempo_tran[IsWinner == 0]) # No
shapiro.test(tempo_tran[IsWinner == 1]) # No
# valence, pretty much the same
par(mfrow = c(1, 1))
b_valence <- boxcox(lm(valence ~ 1))
lambda <- b_valence$x[which.max(b_valence$y)]
valence_tran <- (valence ^ lambda - 1) / lambda
par(mfrow = c(1, 2))
qqnorm(valence_tran[IsWinner == 0])
grid()
qqline(valence_tran[IsWinner == 0],lwd = 2, col = "red")
qqnorm(valence_tran[IsWinner == 1])
grid()
qqline(valence_tran[IsWinner == 1],lwd = 2, col = "red")
shapiro.test(valence_tran[IsWinner == 0]) # No
shapiro.test(valence_tran[IsWinner == 1]) # No
##########################################
par(mfrow = c(1, 1))
b_data_followers <- boxcox(lm(data$followers ~ 1))
lambda <- b_data_followers$x[which.max(b_data_followers$y)]
data_followers_tran <- (data$followers ^ lambda - 1) / lambda
b_data_acousticness <- boxcox(lm(data$acousticness ~ 1))
lambda <- b_data_acousticness$x[which.max(b_data_acousticness$y)]
data_acousticness_tran <- (data$acousticness ^ lambda - 1) / lambda
b_data_duration_ms <- boxcox(lm(data$duration_ms ~ 1))
lambda <- b_data_duration_ms$x[which.max(b_data_duration_ms$y)]
data_duration_ms_tran <- (data$duration_ms ^ lambda - 1) / lambda
b_data_liveness <- boxcox(lm(data$liveness ~ 1))
lambda <- b_data_liveness$x[which.max(b_data_liveness$y)]
data_liveness_tran <- (data$liveness ^ lambda - 1) / lambda
neg_loudness = data$loudness * (-1)
b_data_loudness <- boxcox(lm(neg_loudness ~ 1))
lambda <- b_data_loudness$x[which.max(b_data_loudness$y)]
data_loudness_tran <- (neg_loudness ^ lambda - 1) / lambda
b_data_tempo <- boxcox(lm(data$tempo ~ 1))
lambda <- b_data_tempo$x[which.max(b_data_tempo$y)]
data_tempo_tran <- (data$tempo ^ lambda - 1) / lambda
tran_data = matrix(c(data$IsWinner, data_followers_tran, data_acousticness_tran, data_duration_ms_tran,
data$energy, data$instrumentalness, data_liveness_tran, data_loudness_tran,
data_tempo_tran, data$valence), ncol = 10)
training_tran_data = tran_data[train_ind,]
test_tran_data = tran_data[-train_ind,]
colnames_tran_data = c("IsWinner", "followers_tran", "acousticness_tran", "duration_ms_tran", "energy",
"instrumentalness", "liveness_tran", "loudness_tran", "tempo_tran", "valence")
colnames(training_tran_data) = colnames_tran_data
colnames(test_tran_data) = colnames_tran_data
colnames(tran_data) = colnames_tran_data
training_tran_data = as.data.frame(training_tran_data)
test_tran_data = as.data.frame(test_tran_data)
tran_data = as.data.frame(tran_data)
## LDA
simple_lda = lda(IsWinner ~ followers_tran + acousticness_tran + duration_ms_tran +
energy + instrumentalness + liveness_tran + loudness_tran + tempo_tran +
valence, data = training_tran_data, family = "binomial")
pred_simple_lda = predict(simple_lda, newdata = test_tran_data, type = "Response")
# ROC curve
roc.out <- roc(test_set$IsWinner, as.numeric(pred_simple_lda$class) - 1)
plot(roc.out, print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")
auc(roc.out)
# Oversampled LDA
oversampled_train_tran_data = ovun.sample(IsWinner ~., data = training_tran_data, method = "over", p = 0.5, seed = 42)$data
simple_over_lda = lda(IsWinner ~ followers_tran + acousticness_tran + duration_ms_tran +
energy + instrumentalness + liveness_tran + loudness_tran + tempo_tran +
valence, data = oversampled_train_tran_data, family = "binomial")
pred_simple_over_lda = predict(simple_over_lda, newdata = test_tran_data, type = "Response")
# ROC curve
roc.out <- roc(test_set$IsWinner, as.numeric(pred_simple_over_lda$class) - 1)
plot(roc.out, print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")
auc(roc.out)
# Predictions
# Threshold 0.4
lda_over_predictions_04 = ifelse(pred_simple_over_lda$posterior[,1] > 0.4, 1, 0)
lda_over_accuracy_04 = sum(lda_over_predictions_04 == test_set[2]) / dim(test_set[2])[1]
table(test_set$IsWinner, lda_over_predictions_04)
false_positive_lda_over_04 = table(test_set$IsWinner, lda_over_predictions_04)[3]
negative_lda_over_04 = table(test_set$IsWinner, lda_over_predictions_04)[1] + table(test_set$IsWinner, lda_over_predictions_04)[2]
typeIerror_lda_over_04 = false_positive_lda_over_04 / negative_lda_over_04
true_positive_lda_over_04 = table(test_set$IsWinner, lda_over_predictions_04)[4]
positive_lda_over_04 = table(test_set$IsWinner, lda_over_predictions_04)[2] + table(test_set$IsWinner, lda_over_predictions_04)[4]
sensitivity_lda_over_04 = true_positive_lda_over_04 / positive_lda_over_04
# Threshold 0.5
lda_over_predictions_05 = list(pred_simple_over_lda$class)
lda_over_accuracy_05 = sum(lda_over_predictions_05 == test_set[2]) / dim(test_set[2])[1]
table(test_set$IsWinner, lda_over_predictions_05[[1]])
false_positive_lda_over_05 = table(test_set$IsWinner, lda_over_predictions_05[[1]])[3]
negative_lda_over_05 = table(test_set$IsWinner, lda_over_predictions_05[[1]])[1] + table(test_set$IsWinner, lda_over_predictions_05[[1]])[2]
typeIerror_lda_over_05 = false_positive_lda_over_05 / negative_lda_over_05
true_positive_lda_over_05 = table(test_set$IsWinner, lda_over_predictions_05[[1]])[4]
positive_lda_over_05 = table(test_set$IsWinner, lda_over_predictions_05[[1]])[2] + table(test_set$IsWinner, lda_over_predictions_05[[1]])[4]
sensitivity_lda_over_05 = true_positive_lda_over_05 / positive_lda_over_05
## QDA
qda = qda(IsWinner ~ followers_tran + acousticness_tran + duration_ms_tran +
energy + instrumentalness + liveness_tran + loudness_tran + tempo_tran +
valence, data = training_tran_data, family = "binomial")
pred_qda = predict(qda, newdata = test_tran_data, type = "Response")
# ROC
roc.out <- roc(test_set$IsWinner, as.numeric(pred_qda$class) - 1)
plot(roc.out, print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")
auc(roc.out)
# QDA oversampled
qda_over = qda(IsWinner ~ followers_tran + acousticness_tran + duration_ms_tran +
energy + instrumentalness + liveness_tran + loudness_tran + tempo_tran +
valence, data = oversampled_train_tran_data, family = "binomial")
pred_qda_over = predict(qda_over, newdata = test_tran_data, type = "Response")
# ROC
roc.out <- roc(test_set$IsWinner, as.numeric(pred_qda_over$class) - 1)
plot(roc.out, print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")
auc(roc.out)
# Predictions
# Threshold 0.4
qda_over_predictions_04 = ifelse(pred_qda_over$posterior[,1] > 0.4, 1, 0)
qda_over_accuracy_04 = sum(qda_over_predictions_04 == test_set[2]) / dim(test_set[2])[1]
table(test_set$IsWinner, qda_over_predictions_04)
false_positive_qda_over_04 = table(test_set$IsWinner, qda_over_predictions_04)[3]
negative_qda_over_04 = table(test_set$IsWinner, qda_over_predictions_04)[1] + table(test_set$IsWinner, qda_over_predictions_04)[2]
typeIerror_qda_over_04 = false_positive_qda_over_04 / negative_qda_over_04
true_positive_qda_over_04 = table(test_set$IsWinner, qda_over_predictions_04)[4]
positive_qda_over_04 = table(test_set$IsWinner, qda_over_predictions_04)[2] + table(test_set$IsWinner, qda_over_predictions_04)[4]
sensitivity_qda_over_04 = true_positive_qda_over_04 / positive_qda_over_04
# Threshold 0.5
qda_over_predictions_05 = list(pred_qda_over$class)
qda_over_accuracy_05 = sum(qda_over_predictions_05 == test_set[2]) / dim(test_set[2])[1]
table(test_set$IsWinner, qda_over_predictions_05[[1]])
false_positive_qda_over_05 = table(test_set$IsWinner, qda_over_predictions_05[[1]])[3]
negative_qda_over_05 = table(test_set$IsWinner, qda_over_predictions_05[[1]])[1] + table(test_set$IsWinner, qda_over_predictions_05[[1]])[2]
typeIerror_qda_over_05 = false_positive_qda_over_05 / negative_qda_over_05
true_positive_qda_over_05 = table(test_set$IsWinner, qda_over_predictions_05[[1]])[4]
positive_qda_over_05 = table(test_set$IsWinner, qda_over_predictions_05[[1]])[2] + table(test_set$IsWinner, qda_over_predictions_05[[1]])[4]
sensitivity_qda_over_05 = true_positive_qda_over_05 / positive_qda_over_05
#########################################Ã 
# Regularized regression
## Ridge regression
ridge_cv = cv.glmnet(data.matrix(oversampled_train_data[,-1]), oversampled_train_data$IsWinner,
alpha = 0, family = "binomial", type.measure = "class")
plot(ridge_cv)
pred_ridge = predict(ridge_cv, data.matrix(test_set[, c(-1, -2)]), type = "class", s = lambda_opt_ridge)
