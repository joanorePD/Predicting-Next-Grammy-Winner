---
title: "What Makes a Rock Song Award-Winning?"
author: "Cristian Granchelli, Joan Orellana Rios, Cameron Kelahan"
date: "2023-08-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Can we use attributes and metrics of rock songs to predict their likelihood of winning the Grammy for "Best Rock Song?"

What common traits, if any, do award winning rock songs contain? Can we look at intrinsic traits of songs, combined with metrics defined by Spotify, to determine award winning musical features? In this paper, we break down our data collection, data processing, and data analysis of a dataset containing roughly 1,000 popular songs, both award-winning and not.

## The Data

### Data Collection and Preparation

In order to investigate this, we first collected a list of every song nominated for the "Best Rock Song" Grammy. The reason that we kept it within one award, rather than include other rock-related Grammy's ("Best Rock Performance," "Best Rock Album," etc.), is because they are disparate by nature, and nominations are awarded for completely different reasons. The Best Rock Song Grammy competition is between individual songs, allowing us to compare them to each other and determine what attributes mold a work worthy of a nomination.

In order to examine what sets these nominations apart from other rock songs, we collected the top 10 most streamed songs of each of these previously nominated artists on Spotify so that we could compare the features and attributes of their popular non-nominated songs to songs that earned a nomination. This created a dataset of 1,016 popular rock songs. As a final step of processing the data, we had to remove any songs that were released prior to the creation of this Grammy in 1992. Removing all songs older than 1992 yielded a final dataset of 867 songs.

```{r warning = FALSE, echo = TRUE, message = FALSE}
# Set the working directory to this file's folder
library("rstudioapi")
setwd(dirname(getActiveDocumentContext()$path))
load("final_df_n_str.RData")

Sys.setenv(LANG = "en") 

# Load necessary libraries
library(pROC)
library(MASS)
library(ROSE)
library(confintr)
library(ggplot2)
library(correlation)
library(corrplot)
library(class)
library(caret)
library(glmnet)
```

```{r, echo = TRUE, results = 'hide'}
# Selecting the relevant variables
data = final_df_n_str
data = data[,c("track_name", "artist_name", "IsWinner", "Year","year",
               "followers", "acousticness", "danceability", "duration_ms",
               "energy", "instrumentalness", "key", "liveness", "loudness",
               "mode", "tempo", "time_signature", "valence")]

# Merge the two year variable
data$Year[data$Year == "Undefined"] <- data$year[data$Year == "Undefined"]
data = data[,c("track_name","artist_name", "IsWinner", "Year", "followers",
               "acousticness", "danceability", "duration_ms",
               "energy", "instrumentalness", "key", "liveness", "loudness",
               "mode", "tempo", "time_signature", "valence")]

# Eliminating duplicates
data$track_name == "Closing Time"
data$track_name == "Smells Like Teen Spirit"
data$track_name == "Don't Wanna Fight"
data[914, ]
data[789,]
data[669,]

data = data[-c(669, 789, 914),]

sum(data$Year < 1992)
nrow(data)
data = data[!data$Year < 1992,]

# Creating row names

names = paste0(data$track_name, " - ", data$artist_name)

# Eliminating unusable variables
data = data[,c("IsWinner", "Year", "acousticness",
               "danceability", "duration_ms", "energy",
               "instrumentalness", "key", "loudness", "mode",
               "tempo", "time_signature", "valence")]
data = cbind(names = names, data)

# Casting variables
data$IsWinner[data$IsWinner == "Winner"] = 1
data$IsWinner[data$IsWinner == "Nominee"] = 1
data$IsWinner[data$IsWinner == "Nothing"] = 0
data$IsWinner = as.integer(data$IsWinner)
data$Year = as.integer(data$Year)
data$mode = as.factor(data$mode)
data$key = as.factor(data$key)
data$time_signature = as.factor(data$time_signature)

summary(data)
```

```{r}
# Checking balance between classes

# Non-Grammy nominated songs
length(data$IsWinner[data$IsWinner == 0]) / (length(data$IsWinner[data$IsWinner == 0]) + length(data$IsWinner[data$IsWinner == 1]))

# Grammy nominated songs
length(data$IsWinner[data$IsWinner == 1]) / (length(data$IsWinner[data$IsWinner == 0]) + length(data$IsWinner[data$IsWinner == 1]))
```

From the balance check between the number of nominated and non-nominated songs, it is seen that the classes are highly imbalanced: 82% non-nominated songs, 18% nominated songs. We make note of this during the data processing stage so that we may take measures to address this during our analysis.

### Explanation of Variables

In order to perform analysis of the songs, we decided to use metrics that are intrinsic to music as well as artificial metrics created and measured by the music streaming giant Spotify. The intrinsic metrics we used were: duration, musical key, modality (major or minor key), tempo, and time signature. Spotify also uses what they call ["audio features"](https://developer.spotify.com/documentation/web-api/reference/get-several-audio-features) (in the table below) to perform their own analysis of songs when creating playlists, suggesting music, etc. We used these professionally manufactured metrics to bolster the intrinsic metrics and increase our insight into what might make a song award-winning.

Audio Feature | Definition
------------- | -------------
Acousticness  | A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.
Danceability  | Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.
Energy        | Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.
Instrumentalness | Predicts whether a track contains no vocals. "Ooh" and "aah" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly "vocal". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.
Loudness      | The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.
Valence       | A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).

Although Spotify does not openly share the formulas behind how they determine these metrics, we found them suitable to assist in our analysis.

As a final processing step, we split the data into training and test datasets. The training dataset contains 80% of the original dataset, and the remaining 20% of the data in the test dataset will be used to test our model against after we have trained it. It is very important to test the model on never-before-seen data to determine not only how well the model performs, but also how well the model can generalize.

```{r, echo = TRUE, eval = TRUE}
# Splitting training and test set
training_size = floor(0.8 * nrow(data))
set.seed(42)
train_ind = sample(seq_len(nrow(data)), size = training_size)
training_set = data[train_ind,]
test_set = data[-train_ind,]

summary(training_set)

# Checking if the ratio is preserved

# Ratio of training set:
# - not-nominated
length(training_set$IsWinner[data$IsWinner == 0]) / (length(training_set$IsWinner[data$IsWinner == 0]) + length(training_set$IsWinner[data$IsWinner == 1]))
# - nominated
length(training_set$IsWinner[data$IsWinner == 1]) / (length(training_set$IsWinner[data$IsWinner == 0]) + length(training_set$IsWinner[data$IsWinner == 1]))

# Ratio of Test Set
# - not-nominated
length(test_set$IsWinner[data$IsWinner == 0]) / (length(test_set$IsWinner[data$IsWinner == 0]) + length(test_set$IsWinner[data$IsWinner == 1]))
# - nominated
length(test_set$IsWinner[data$IsWinner == 1]) / (length(test_set$IsWinner[data$IsWinner == 0]) + length(test_set$IsWinner[data$IsWinner == 1]))
```

## Exploratory Data Analysis

### Relationship Between Independent Variables

At first, correlations between the independent variables were examined, which is the measurement of association between two variables. This is essentially a measurement of how similar the information we learn from different variables are. The larger the correlation between independent variables (negative or positive), the higher their collinearity, meaning the information they each carry is similar. This means their use together is less beneficial due to redundancy and should avoid the pairing. It is preferable to utilize pairings of variables that offer different information to assist the model.

It was important to understand the data better in these regards, so that it can be known which attribute pairings could give useful insight.

#### Correlation Between Contiuous Variables

```{r, message = FALSE}
attach(training_set)
# Correlations between continuous variables
cor_matrix = cor(training_set[,c(-1, -2, -9, -11, -13)])
```
```{r, eval=FALSE}
# Plot the correlation
corrplot.mixed(cor_matrix, tl.pos='lt')
```

```{r, echo=FALSE}
knitr::include_graphics("corplot_indep_1.png")
```

The above plot visualizes the correlations between the continuous, independent variables. There are four pairings of variables that produce a large correlation: acousticness/energy, acousticness/loudness, danceability/valence, and loudness/energy. It was interesting to see that loudness and energy had such a high correlation, giving us some insight into how these metrics are defined.

```{r, eval = FALSE}
pairs(training_set[,c(-1, -2, -9, -11, -13)], lower.panel = panel.smooth)
```

```{r, echo=FALSE}
knitr::include_graphics("corplot_indep_2.png")
```

From evaluating this second type of plot showing the correlation between continuous, independent variables, one can understand the shape of the data and discover outliers as well as general trends. For example, almost the entire trend line of duration vs instrumentalness is caused by a single outlier. It also confirmed the high, positive correlation between loudness and energy, showing a nice upward direction of the trend line with the data points clustered around it. Furthermore, it showed the strong trend line between valence and danceability, although those data have higher variability and are less centered around the line. The highly negative correlation between acousticness and energy is also visualized, touting a firmly downward trend line.

#### Correlation Between Categorical Variables

In order to determine the strength of similarity between the categorical variables, we employed the Cramer's V measurement. This measurement is a normalized version of the chi-square statistic, returning a value of 0 if there is no association between the categorical variables, and returning 1 if there is a perfect association in which one variable is completely determined by the other.

```{r, warning=FALSE}
# Association measure for categorical variables (Cramer's V is a normalized 
# version of the chi-square statistics)

# Key / Mode
cramersv(matrix(c(as.numeric(key), as.numeric(mode)), ncol = 2))
# Key / Time Signature
cramersv(matrix(c(as.numeric(key), as.numeric(time_signature)), ncol = 2))
# Mode / Time Signature
cramersv(matrix(c(as.numeric(mode), as.numeric(time_signature)), ncol = 2))
```

Although measurements of association between categorical variables are hard to interpret, the Cramer's V test still illuminates if there is full disassociation or full association between said variables. From the data, it can be seen that key/mode and key/time_signature are definitely not fully disassociated or fully associated. The same can be said about mode/time_signature, although the measured value was closer to 0.

#### ANOVA Test

Next, associations between each of the categorical variables and all of the continuous variables were examined utilizing ANOVA. ANOVA (or ANalysis Of VAriation) offers a way to compare continuous and categorical variables. If ANOVA points out some feature combinations are significant, then that suggests the statistical means of the groups of continuous variables are different. The chosen threshold for significance was 5%.

ANOVA makes some assumptions regarding the variances of the attributes which are not fully satisfied by all of our variables, meaning the obtained results should be considered carefully.

```{r, warnings = FALSE}
# Association between continuous and categorical variables

# Key
aco_key.aov <- aov(acousticness ~ key)
summary(aco_key.aov)

dan_key.aov <- aov(danceability ~ key)
summary(dan_key.aov) # SIGNIFICANT

dur_key.aov <- aov(duration_ms ~ key)
summary(dur_key.aov)

ene_key.aov <- aov(energy ~ key)
summary(ene_key.aov) # SIGNIFICANT

ins_key.aov <- aov(instrumentalness ~ key)
summary(ins_key.aov)

loud_key.aov <- aov(loudness ~ key)
summary(loud_key.aov)

tem_key.aov <- aov(tempo ~ key)
summary(tem_key.aov)

val_key.aov <- aov(valence ~ key)
summary(val_key.aov)


# Mode
aco_mode.aov <- aov(acousticness ~ mode)
summary(aco_mode.aov) # SIGNIFICANT

dan_mode.aov <- aov(danceability ~ mode)
summary(dan_mode.aov)

dur_mode.aov <- aov(duration_ms ~ mode)
summary(dur_mode.aov)

ene_mode.aov <- aov(energy ~ mode)
summary(ene_mode.aov) # SIGNIFICANT

ins_mode.aov <- aov(instrumentalness ~ mode)
summary(ins_mode.aov)

loud_mode.aov <- aov(loudness ~ mode)
summary(loud_mode.aov) # SIGNIFICANT

tem_mode.aov <- aov(tempo ~ mode)
summary(tem_mode.aov)

val_mode.aov <- aov(valence ~ mode)
summary(val_mode.aov)


# Time signature
aco_time.aov <- aov(acousticness ~ time_signature)
summary(aco_time.aov) # SIGNIFICANT

dan_time.aov <- aov(danceability ~ time_signature)
summary(dan_time.aov) # SIGNIFICANT

dur_time.aov <- aov(duration_ms ~ time_signature)
summary(dur_time.aov) # SIGNIFICANT

ene_time.aov <- aov(energy ~ time_signature)
summary(ene_time.aov) # SIGNIFICANT

ins_time.aov <- aov(instrumentalness ~ time_signature)
summary(ins_time.aov) # SIGNIFICANT

loud_time.aov <- aov(loudness ~ time_signature)
summary(loud_time.aov) # SIGNIFICANT

tem_time.aov <- aov(tempo ~ time_signature)
summary(tem_time.aov) # SIGNIFICANT

val_time.aov <- aov(valence ~ time_signature)
summary(val_time.aov) # SIGNIFICANT
```

The table below lists the categorical/continuous variable combinations that may be significant, according to ANOVA: 

Categorical    | Continuous
---------------| :------------------------------------------:
Key            | Danceability;  Energy
Mode           | Acousticness;  Energy;  Loudness
Time signature | Acousticness;  Danceability;  Duration;  Energy;  Instrumentalness;  Loudness;  Tempo;  Valence

The fact that these groups were significant allows us to say there may be a strong enough correlation between these pairings to use them in our models in a beneficial manner. 

Interestingly, the time signature variable shows significance in combination with all other variables. This may be due to the fact that this variable is highly imbalanced, since 93.4% of the songs have a time signature of 4/4.

#### Partial Correlations

Because ANOVA does not convey which means were different from the others, it is necessary to perform a Multiple Comparison Procedure. To do this, we utilized partial correlations, an exhaustive search that compares each possible pairing of continuous variables.

```{r, warnings = FALSE}
# Partial correlations
correlation(training_set[,c(-1, -2, -9, -11, -13)], partial = TRUE)
```

There were three variable pairs that stood out as highly significantly different:

```{r, warnings = FALSE}
# Plots of variables with the largest partial correlation

# Danceability / Valence
ggplot(data = training_set, aes(danceability, valence)) + geom_jitter(color = "blue")
# Loudness / Energy
ggplot(data = training_set, aes(loudness, energy)) + geom_jitter(color = "blue")
# Acousticness / Energy
ggplot(data = training_set, aes(acousticness, energy)) + geom_jitter(color = "blue")
```

These three pairings are the same three largest correlations seen from the first correlation plots, confirming those findings. It is also interesting to see how the correlation value changes, THEY ARE HIGHER  HERE; ASK CRISTIAN MORE ABOUT THIS.

```{r}
#Weird song, very long; outlier 
which.max(data$duration_ms)
data[448, ]
```

The song Play by Dave Grohl is nearly 23 minutes long. We decided not to delete this song, even though it could be considered an outlier, since it can give meaningful insight on one of the reasons why a song may not gain popularity. Indeed, a song that is really long is less likely to be played on the radio or used in movies, so it may not be heard by many people. This can later influence its likelihood of being considered for a grammy nomination. 

By definition, correlation speaks to linear relationships. This information will directly guide how we develop linear models of our data, but non-linear relationships between the highly correlated data points could still exist.

### Distributions

We look at distributions to better understand the data and how we can interact with it. Some models we want to use are based on assumptions that are related to distributions, often times assuming a normal distribution of data. From gauging how different variables are distributed, we can create better models by taking their distribution assumptions into account.

#### Continuous

```{r, warnings = FALSE, eval=FALSE}
# Checking distributions

par(mfrow= c(2, 4))
 
# Continuous variables
hist(acousticness)
hist(danceability)
hist(duration_ms)
hist(energy)
hist(instrumentalness)
hist(loudness)
hist(tempo)
hist(valence)
```

```{r, echo=FALSE}
knitr::include_graphics("distrib_contin_hist.png")
```

From these histograms, it is clear that the danceability feature is the most normally distributed. All others have a small skew at best (valence) to larger skews (tempo, loudness, energy) to seemingly exponential skews or individual values (duration, acousticness, instrumentalness). As we build our models, we will keep these distributions in mind.

We also compared the distributions and means of each variable for non-nominated and nominated songs. Most were very similar, but below we highlighted the attribute with the greatest difference: Valence.

```{r}
# Comparison IsWinner 0 vs 1 - Valence

par(mfrow = c(1, 2))

x <- data[data$IsWinner == 0,]$valence
hist(x, main = "Valence: Non-Nominated", xlab="Value")
abline(v = mean(x),                       # Add line for mean
       col = "red",
       lwd = 3)
text(x = mean(x) * 1.5,                   # Add text for mean
     y = 121,
     paste("Mean =", round(mean(x), digits=2)),
     col = "red",
     cex = 1)

x <- data[data$IsWinner == 1,]$valence
hist(x, main = "Valence: Nominated", xlab="Value")
abline(v = mean(x),                       # Add line for mean
       col = "red",
       lwd = 3)
text(x = mean(x) * 0.5,                   # Add text for mean
     y = 31,
     paste("Mean =", round(mean(x), digits=2)),
     col = "red",
     cex = 1)
```

The valence variable peaks at different locations for the two values of the dependent variable and has a different mean. This could point towards valence being an important factor in our predictions.

#### Categorical

```{r}
# Categorical variables

par(mfrow = c(1, 3))

barplot(table(key), main = "Key distribution")
barplot(table(mode), main = "Modality")
barplot(table(time_signature), main = "Time signature")
```

None of the categorical variables exhibit a normal distribution. Both mode and time_signature have an obvious statistical mode. The value of 1 for the variable mode represents songs in major keys, making up a super majority of the songs, and almost every song uses the time_signature of 4 with only a handful using 3. With such high frequencies of values within these categories... WHAT DOES IT SAY

We were again curious to compare the distribution of non-nominated to nominated songs for the categorical variables, and highlight the one that demonstrated the greatest difference: Key.

```{r}
# Comparison IsWinner 0 vs 1 - Key

par(mfrow = c(1, 2))
x <- data[data$IsWinner == 0,]$key
barplot(table(x), main = "Key: Non-Nominated")

x <- data[data$IsWinner == 1,]$key
barplot(table(x), main = "Key: Nominated/Winner")
```

Although the distribution of key for both values of the dependent variable look very similar, there are some important differences. We see a much larger proportion of non-nominated songs written in key 4 (corresponding to the key of E) and key 7 (corresponding to the key of G) when compared to nominated songs. The opposite can be said about key 11 (key of B), with a larger proportion of nominated songs in that key when compared to non-nominated songs.

While these patterns are observable, the effect size may not be substantial enough to reach statistical significance, as we will see in the Chi-Squared tests performed. In other words, while key '11' may appear more often in nominated songs, the difference may not be large enough to confidently conclude that it has a significant impact on nominations.

#### Relationship Between Independent and Dependent Variables

Examining the association between independent and dependent variables can give insight into the data of independent variables which exhibit notable differences between the two possible values of the dependent variable.

##### Continuous

Box plots were utilized to visualize these relationships:

```{r}
# Relationships between dependent and independent variables

par(mfrow= c(2, 4))

boxplot(danceability ~ training_set$IsWinner, xlab='Nominee Boolean')
boxplot(acousticness ~ training_set$IsWinner, xlab='Nominee Boolean')
boxplot(duration_ms ~ training_set$IsWinner, xlab='Nominee Boolean')
boxplot(energy ~ training_set$IsWinner, xlab='Nominee Boolean')
boxplot(instrumentalness ~ training_set$IsWinner, xlab='Nominee Boolean')
boxplot(loudness ~ training_set$IsWinner, xlab='Nominee Boolean')
boxplot(tempo ~ training_set$IsWinner, xlab='Nominee Boolean')
boxplot(valence ~ training_set$IsWinner, xlab='Nominee Boolean')
```

The difference in median seen for the energy and valence variables points towards them being possibly valuable to our models. The difference in interquartile range for acousticness, energy, and valence is also interesting to see.

##### Categorical

To measure the independent/dependent variables association for the categorical variables, we utilized the Chi Squared Test. It will pass judgement on the level of independence between the variables.

```{r, warning=FALSE, message=FALSE}
par(mfrow = c(1, 1))

chisq.test(key, training_set$IsWinner)
chisq.test(mode, training_set$IsWinner)
chisq.test(time_signature, training_set$IsWinner)
```

The Chi Squared Test revealed fairly high p-values for all of the categorical variables, and even produced a 1 for mode suggesting complete independence of mode from the dependent variable.

We also utilize the Cramer's V again, and it showed.... EXPAND

```{r}
cramersv(matrix(c(as.numeric(key), as.numeric(training_set$IsWinner)), ncol = 2))
cramersv(matrix(c(as.numeric(mode), as.numeric(training_set$IsWinner)), ncol = 2))
cramersv(matrix(c(as.numeric(time_signature), as.numeric(training_set$IsWinner)), ncol = 2))
```

#### Summary

Correlations between continuous variables were found, specially high correlations between audio features such as acousticness/energy and loudness/energy. However no categorical variables were shown to have any significant association with the dependent variable.

We examined the distribution for both continuous and categorical variables. Most continuous variables showed non-normal distributions, which should be considered when building models. Categorical variables like mode and time signature had high frequencies of specific values.

For continuous variables, we used box plots to visualize differences between nominated and non-nominated songs. Notably, energy and valence showed differences in medians. For categorical variables, Chi-Squared tests and Cramer's V indicated that these variables might not be strongly associated with the Grammy nomination status.

From this analysis we can extract that features like energy and valence may play a role in distinguishing nominated from non-nominated songs. On the other hand, categorical variables didn't showed significant impact on nominations. This information is surely valuable for the next step, model development.

## Model Development

Explain our approach to modeling:

- Start with linear, move on to... etc.

Because of the imbalanced nature of the dataset that was shown previously, an oversampled version of the dataset will be used in an attempt to further improve our models. This oversampling creates synthetic data of the minority class (nominated) with data values for all variables similar to and determined by the real data points of the minority class. This creates a dataset of balanced non-nominated and nominated songs.

```{r}
## Oversampling

oversampled_train_data = ovun.sample(IsWinner ~.,
                                     data = training_set[,-1],
                                     method = "over",
                                     p = 0.5, seed = 42)$data

# Checking oversampled training set balance

sum(oversampled_train_data$IsWinner == 0)
sum(oversampled_train_data$IsWinner == 1)
```

### Logistic Model

The first model created in this project was a logistic model, being a straightforward binary classification tool that will classify the songs as non-nominated or nominated works. It considers all the variables.

```{r}
## Simple logistic model

logistic = glm(IsWinner ~ ., data = training_set[,c(-1,-2)],
               family = "binomial")
summary(logistic)
```

From the creation of the model, it determined valence, duration, and acousticness to be significant variables.

#### Predictions

```{r}
# Computing predictions

logistic_predictions_full = predict(logistic,
                                    newdata = test_set[,c(-1, -2)],
                                    type = "response")
```

Below are the predictions as computed by the logistic model created above, using different thresholds of classification. The threshold defines at which confidence level the model will classify a song as "nominated."

##### Threshold = 0.2

```{r}
# Threshold = 0.2

logistic_predictions_full_02 = ifelse(logistic_predictions_full > 0.2, 1, 0)
logistic_accuracy_full_02 = sum(logistic_predictions_full_02 == test_set[2]) / dim(test_set[2])[1]

table(test_set$IsWinner, logistic_predictions_full_02)

false_positive_logistic_full_02 = table(test_set$IsWinner, logistic_predictions_full_02)[3]
negative_logistic_full_02 = table(test_set$IsWinner, logistic_predictions_full_02)[1] + table(test_set$IsWinner, logistic_predictions_full_02)[3]
typeIerror_logistic_full_02 = false_positive_logistic_full_02 / negative_logistic_full_02

typeIerror_logistic_full_02
```

```{r}
true_positive_logistic_full_02 = table(test_set$IsWinner, logistic_predictions_full_02)[4]
positive_logistic_full_02 = table(test_set$IsWinner, logistic_predictions_full_02)[2] + table(test_set$IsWinner, logistic_predictions_full_02)[4]
sensitivity_logistic_full_02 = true_positive_logistic_full_02 / positive_logistic_full_02

sensitivity_logistic_full_02
```

At a classification threshold of 0.2, the Type 1 Error (False Positive Rate), defined as False Positive classifications / Total Negative songs, was 0.32. That means around a third of the non-nominated songs were misclassified as nominated 

The Sensitivity (True Positive Rate), defined as True Positive classifications / Total positive songs was 0.5. Contextually, this means that half of the nominated songs were classified correctly.

Ideally, the Type 1 error should be lower than this and the Sensitivity should be much higher.

##### Threshold = 0.3

In an attempt to improve the classification of songs from our model, the threshold was increased to 0.3.

```{r}
# Threshold = 0.3

logistic_predictions_full_03 = ifelse(logistic_predictions_full > 0.3, 1, 0)
logistic_accuracy_full_03 = sum(logistic_predictions_full_03 == test_set[2]) / dim(test_set[2])[1]

table(test_set$IsWinner, logistic_predictions_full_03)

false_positive_logistic_full_03 = table(test_set$IsWinner, logistic_predictions_full_03)[3]
negative_logistic_full_03 = table(test_set$IsWinner, logistic_predictions_full_03)[1] + table(test_set$IsWinner, logistic_predictions_full_03)[3]
typeIerror_logistic_full_03 = false_positive_logistic_full_03 / negative_logistic_full_03

typeIerror_logistic_full_03
```

```{r}
true_positive_logistic_full_03 = table(test_set$IsWinner, logistic_predictions_full_03)[4]
positive_logistic_full_03 = table(test_set$IsWinner, logistic_predictions_full_03)[2] + table(test_set$IsWinner, logistic_predictions_full_03)[4]
sensitivity_logistic_full_03 = true_positive_logistic_full_03 / positive_logistic_full_03

sensitivity_logistic_full_03
```

With an increase in threshold to 0.3, the Type 1 Error decreased to 0.113. This was an improvement, as it means there were fewer non-nominated songs classified as nominated, but to get a full picture Sensitivity still needed to be examined.

The Sensitivity also decreased to 0.083, which is not optimal behavior. This means that at a classification threshold of 0.3, the model correctly predicts only 8% of the nominated songs.

The combined information from these two tests suggests the model is classifying more songs as non-nominated, avoiding false positives while also missing correct classifications of nominated songs.

##### Threshold = 0.4

For a final comparison, the threshold was set to 0.4.

```{r}
# Threshold = 0.4

logistic_predictions_full_04 = ifelse(logistic_predictions_full > 0.4, 1, 0)
logistic_accuracy_full_04 = sum(logistic_predictions_full_04 == test_set[2]) / dim(test_set[2])[1]

table(test_set$IsWinner, logistic_predictions_full_04)

false_positive_logistic_full_04 = table(test_set$IsWinner, logistic_predictions_full_04)[3]
negative_logistic_full_04 = table(test_set$IsWinner, logistic_predictions_full_04)[1] + table(test_set$IsWinner, logistic_predictions_full_04)[3]
typeIerror_logistic_full_04 = false_positive_logistic_full_04 / negative_logistic_full_04

typeIerror_logistic_full_04
```

```{r}
true_positive_logistic_full_04 = table(test_set$IsWinner, logistic_predictions_full_04)[4]
positive_logistic_full_04 = table(test_set$IsWinner, logistic_predictions_full_04)[2] + table(test_set$IsWinner, logistic_predictions_full_04)[4]
sensitivity_logistic_full_04 = true_positive_logistic_full_04 / positive_logistic_full_04

sensitivity_logistic_full_04
```

With an increase in threshold to 0.4, the Type 1 Error decreased further to 0.013. This again was an improvement, but it may be detrimental to the overall model accuracy.

The Sensitivity reached 0. This means the model managed to properly classify 0 nominated songs

Overall, increasing the threshold was detrimental.

#### ROC Curve

An ROC Curve is meant to display the performance of a classification model. It plots the True Positive Rate (Sensitivity) against the False positive Rate (Type 1 Error) across different confidence thresholds. This takes into account both sides of the classification accuracy we discussed above and paints a comprehensive picture of how well the model can classify the song data.

```{r}
# ROC curve

roc.out <- roc(test_set$IsWinner, logistic_predictions_full)
plot(roc.out, print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")
auc(roc.out)
```

The ROC curve produced by the logistic model touts an Area Under the ROC Curve (AUR/AUC) of 0.598, meaning it achieves almost 60% diagnostic accuracy. A model producing an AUC greater than 50% is needed to make a model more efficient than a coin flip, so the basic logistic model does attain some predictive capabilities.

### Stepwise Variable Selection and Reduced Model

Stepwise variable selection was performed in an attempt to further improve the logistic model. This process removes the least beneficial variable from the model's consideration. As it removes more and more variables, it also goes back and checks if re-adding a previously removed variable would benefit the current state of the model at that time.

```{r}
# Stepwise variable selection

log_both =  stepAIC(logistic, direction = "both")
```

According to the stepwise selection, the most valuable variables for the logistic model are: year, valence, duration, and acousticness.

```{r}
# Fitting the reduced model

logistic_reduced = glm(IsWinner ~  Year + valence + duration_ms + acousticness, data = training_set,  family = "binomial")

summary(logistic_reduced)
```

Utilizing the information gained from the stepwise variable selection, a reduced version of the logistic model was created.

#### Predicting

```{r}
# Computing predictions

logistic_predictions = predict(logistic_reduced, newdata = test_set[,c(-1, -2)], type = "response")
```

As done with the full logistic model, predictions using different thresholds were examined to ascertain the performance of the reduced model.

##### Threshold = 0.2
```{r}
# Threshold = 0.2

logistic_predictions_02 = ifelse(logistic_predictions > 0.2, 1, 0)
logistic_accuracy_02 = sum(logistic_predictions_02 == test_set[2]) / dim(test_set[2])[1]

table(test_set$IsWinner, logistic_predictions_02)

false_positive_logistic_02 = table(test_set$IsWinner, logistic_predictions_02)[3]
negative_logistic_02 = table(test_set$IsWinner, logistic_predictions_02)[1] + table(test_set$IsWinner, logistic_predictions_02)[3]
typeIerror_logistic_02 = false_positive_logistic_02 / negative_logistic_02

typeIerror_logistic_02
```

```{r}
true_positive_logistic_02 = table(test_set$IsWinner, logistic_predictions_02)[4]
positive_logistic_02 = table(test_set$IsWinner, logistic_predictions_02)[2] + table(test_set$IsWinner, logistic_predictions_02)[4]
sensitivity_logistic_02 = true_positive_logistic_02 / positive_logistic_02

sensitivity_logistic_02
```

At a confidence threshold of 0.2, the model produces a Type 1 Error of 0.37, incorrectly classifying 37% of the non-nominated songs as nominated. This is slightly worse than the 0.32 seen from the full model.

It also produces a sensitivity of 0.54, meaning it correctly classifies 54% of the nominated songs, a slight improvement to the 0.5 produced from the full model at this threshold.

##### Threshold = 0.3
```{r}
# Threshold = 0.3

logistic_predictions_03 = ifelse(logistic_predictions > 0.3, 1, 0)
logistic_accuracy_03 = sum(logistic_predictions_03 == test_set[2]) / dim(test_set[2])[1]

table(test_set$IsWinner, logistic_predictions_03)

false_positive_logistic_03 = table(test_set$IsWinner, logistic_predictions_03)[3]
negative_logistic_03 = table(test_set$IsWinner, logistic_predictions_03)[1] + table(test_set$IsWinner, logistic_predictions_03)[3]
typeIerror_logistic_03 = false_positive_logistic_03 / negative_logistic_03

typeIerror_logistic_03
```

```{r}
true_positive_logistic_03 = table(test_set$IsWinner, logistic_predictions_03)[4]
positive_logistic_03 = table(test_set$IsWinner, logistic_predictions_03)[2] + table(test_set$IsWinner, logistic_predictions_03)[4]
sensitivity_logistic_03 = true_positive_logistic_03 / positive_logistic_03

sensitivity_logistic_03
```

As seen before with the full model, increasing the threshold to 0.3 decreased the Type 1 Error, but to a more significant degree. It now produces a value of 0.04, which is a significant drop in misclassifications of non-nominated songs. Coupled with that, however, was a significant drop in Sensitivity, reaching 0.042.

##### Threshold = 0.4
```{r}
# Threshold = 0.4

logistic_predictions_04 = ifelse(logistic_predictions > 0.4, 1, 0)
logistic_accuracy_04 = sum(logistic_predictions_04 == test_set[2]) / dim(test_set[2])[1]

table(test_set$IsWinner, logistic_predictions_04)

false_positive_logistic_04 = table(test_set$IsWinner, logistic_predictions_04)[3]
negative_logistic_04 = table(test_set$IsWinner, logistic_predictions_04)[1] + table(test_set$IsWinner, logistic_predictions_04)[3]
typeIerror_logistic_04 = false_positive_logistic_04 / negative_logistic_04

typeIerror_logistic_04
```

```{r}
true_positive_logistic_04 = table(test_set$IsWinner, logistic_predictions_04)[4]
positive_logistic_04 = table(test_set$IsWinner, logistic_predictions_04)[2] + table(test_set$IsWinner, logistic_predictions_04)[4]
sensitivity_logistic_04 = true_positive_logistic_04 / positive_logistic_04

sensitivity_logistic_04
```

As was done with the full logistic model, the threshold was increased to 0.4. With this change, the model classified no songs as nominated, producing values of N/A from the calculations.

#### ROC Curve

```{r}
# ROC curve

roc.out <- roc(test_set$IsWinner, logistic_predictions)
plot(roc.out, print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")
auc(roc.out)
```

The ROC curve shows that the reduced logistic model produces a diagnostic accuracy of 0.576, slightly worse than the full logistic model.

### Logistic Oversampled Model

In this section, a logistic model is created (same as seen above) using the previously created oversampled data set that was discussed at the beginning of the Model Development section as an attempt to improve the logistic model's accuracy.

```{r}
# Fitting logistic oversampled 

logistic_over = glm(as.numeric(unlist(oversampled_train_data[1])) ~ .,
                    data = oversampled_train_data[-1],
                    family = "binomial")
summary(logistic_over)
```

Using the oversampled dataset, the full logistic model now determines the following variables to be significant: year, acousticness, duration, key5, key11, loudness, and valence. This is quite a difference compared to the previous logistic model based on the original dataset, which only listed 3 variables (year, duration, and valence) as significant.

#### Predictions

```{r}
# Computing predictions

logistic_over_predictions_full = predict(logistic_over, newdata = test_set[,c(-1, -2)], type = "response")
```

Below are the predictions as computed by the oversampled, full logistic model created above, using different thresholds of classification. The same thresholds that were explored in the original-dataset full logistic model will he highlighted again: 0.2, 0.3, and 0.4

##### Threshold = 0.2

```{r}
# Threshold = 0.2

logistic_over_predictions_full_02 = ifelse(logistic_over_predictions_full > 0.2, 1, 0)
logistic_over_accuracy_full_02 = sum(logistic_over_predictions_full_02 == test_set[2]) / dim(test_set[2])[1]

table(test_set$IsWinner, logistic_over_predictions_full_02)

false_positive_logistic_over_full_02 = table(test_set$IsWinner, logistic_over_predictions_full_02)[3]
negative_logistic_over_full_02 = table(test_set$IsWinner, logistic_over_predictions_full_02)[1] + table(test_set$IsWinner, logistic_over_predictions_full_02)[3]
typeIerror_logistic_over_full_02 = false_positive_logistic_over_full_02 / negative_logistic_over_full_02

typeIerror_logistic_over_full_02

true_positive_logistic_over_full_02 = table(test_set$IsWinner, logistic_over_predictions_full_02)[4]
positive_logistic_over_full_02 = table(test_set$IsWinner, logistic_over_predictions_full_02)[2] + table(test_set$IsWinner, logistic_over_predictions_full_02)[4]
sensitivity_logistic_over_full_02 = true_positive_logistic_over_full_02 / positive_logistic_over_full_02

sensitivity_logistic_over_full_02
```

At threshold 0.2, the Type 1 Error (False Positive Rate) was 0.853, meaning 85.3% of the non-nominated songs were misclassified as nominated. This is a fairly large error.

The Sensitivity (True Positive Rate) was .958, a fantastic value, showing the the oversampled full logistic model correctly classified 95.8% of the nominated songs. However, knowing the value of the Type 1 Error, it is clear that this value is so high because the model is classifying a vast majority of the data points as nominated. While it misses very few of the truly nominated songs, it misclassifies a large number of the non-nominated songs.

##### Threshold = 0.3

```{r}
# Threshold = 0.3

logistic_over_predictions_full_03 = ifelse(logistic_over_predictions_full > 0.3, 1, 0)
logistic_over_accuracy_full_03 = sum(logistic_over_predictions_full_03 == test_set[2]) / dim(test_set[2])[1]

table(test_set$IsWinner, logistic_over_predictions_full_03)

false_positive_logistic_over_full_03 = table(test_set$IsWinner, logistic_over_predictions_full_03)[3]
negative_logistic_over_full_03 = table(test_set$IsWinner, logistic_over_predictions_full_03)[1] + table(test_set$IsWinner, logistic_over_predictions_full_03)[3]
typeIerror_logistic_over_full_03 = false_positive_logistic_over_full_03 / negative_logistic_over_full_03

typeIerror_logistic_over_full_03

true_positive_logistic_over_full_03 = table(test_set$IsWinner, logistic_over_predictions_full_03)[4]
positive_logistic_over_full_03 = table(test_set$IsWinner, logistic_over_predictions_full_03)[2] + table(test_set$IsWinner, logistic_over_predictions_full_03)[4]
sensitivity_logistic_over_full_03 = true_positive_logistic_over_full_03 / positive_logistic_over_full_03

sensitivity_logistic_over_full_03
```

At a threshold of 0.3, the Type 1 Error improves to 0.693, misclassifying fewer non-nominated songs than at threshold 0.2, but the Sensitivity also decreases to 0.792 meaning the model is missing more of the nominated songs.

##### Threshold = 0.4

```{r}
# Threshold = 0.4

logistic_over_predictions_full_04 = ifelse(logistic_over_predictions_full > 0.4, 1, 0)
logistic_over_accuracy_full_04 = sum(logistic_over_predictions_full_04 == test_set[2]) / dim(test_set[2])[1]

table(test_set$IsWinner, logistic_over_predictions_full_04)

false_positive_logistic_over_full_04 = table(test_set$IsWinner, logistic_over_predictions_full_04)[3]
negative_logistic_over_full_04 = table(test_set$IsWinner, logistic_over_predictions_full_04)[1] + table(test_set$IsWinner, logistic_over_predictions_full_04)[3]
typeIerror_logistic_over_full_04 = false_positive_logistic_over_full_04 / negative_logistic_over_full_04

typeIerror_logistic_over_full_04

true_positive_logistic_over_full_04 = table(test_set$IsWinner, logistic_over_predictions_full_04)[4]
positive_logistic_over_full_04 = table(test_set$IsWinner, logistic_over_predictions_full_04)[2] + table(test_set$IsWinner, logistic_over_predictions_full_04)[4]
sensitivity_logistic_over_full_04 = true_positive_logistic_over_full_04 / positive_logistic_over_full_04

sensitivity_logistic_over_full_04
```

At a classification threshold of 0.4, the model's Type 1 Error decreases a fair amount to 0.5, but the sensitivity also decreases slightly to 0.75. In comparison to the full logistic model using the original dataset, which at this threshold obtained the worst Sensitivity value of 0, the oversampled full logistic model performs much better (again, at this threshold). The ROC curve will exhibit how well the model performs overall.

#### ROC Curve

```{r}
# ROC curve

roc.out <- roc(test_set$IsWinner, logistic_over_predictions_full)
plot(roc.out, print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")
auc(roc.out)
```

Overall, the ROC curve shows that the full oversampled logistic model produces a diagnostic accuracy of 0.579, which is slightly lower than the ROC curve produced by the full logistic model using the original data.

### Stepwise Variable Selection and Reduced Model of the Oversampled Dataset

Stepwise variable selection is again performed on the oversampled logistic model in an attempt to improve the model's classification accuracy by only utilizing the most insightful variables.

```{r}
log_over_both =  stepAIC(logistic_over, direction = "both")

response_variable_over = as.numeric(unlist(oversampled_train_data[1]))

reduced_variables_over = as.matrix(oversampled_train_data[,c(2, 3, 5, 7, 8, 9, 12, 13)], ncol = 8)

reduced_train_data_over = matrix(c(
  response_variable_over,
  as.numeric(reduced_variables_over[,1]),
  as.numeric(reduced_variables_over[,2]),
  as.numeric(reduced_variables_over[,3]),
  as.numeric(reduced_variables_over[,4]),
  as.factor(reduced_variables_over[,5]),
  as.numeric(reduced_variables_over[,6]),
  as.factor(reduced_variables_over[,7]),
  as.numeric(reduced_variables_over[,8])
), ncol = 9)

head(reduced_variables_over)
```

The stepwise function determined the following variables to be the useful and cohesive: instrumentalness, year, loudness, time_signature, key, valence, acousticness, and duration.

```{r}
colnames(reduced_train_data_over) = c("IsWinner", "Year", "acousticness",
                                      "duration_ms", "instrumentalness",
                                      "key", "loudness",
                                      "time_signature", "valence" )
logistic_reduced_over = glm(response_variable_over ~ Year + acousticness 
                            + duration_ms + instrumentalness + key 
                            + loudness 
                            + time_signature + valence,
                            data = oversampled_train_data,
                            family = "binomial")
```

#### Predictions

```{r}
logistic_reduced_over_predictions = predict(logistic_reduced_over,
                                            newdata = test_set[,c(-1, -2)],
                                            type = "response")
```

In order to compare to the previously created models with ease, the reduced oversampled logistic model was tested on the same three thresholds that the other models were tested on.

##### Threshold = 0.2

```{r}
# Threshold = 0.2

logistic_reduced_over_predictions_02 = ifelse(logistic_reduced_over_predictions > 0.2, 1, 0)
logistic_reduced_over_accuracy_02 = sum(logistic_reduced_over_predictions_02 == test_set[2]) / dim(test_set[2])[1]

table(test_set$IsWinner, logistic_reduced_over_predictions_02)

false_positive_logistic_reduced_over_02 = table(test_set$IsWinner, logistic_reduced_over_predictions_02)[3]
negative_logistic_reduced_over_02 = table(test_set$IsWinner, logistic_reduced_over_predictions_02)[1] + table(test_set$IsWinner, logistic_reduced_over_predictions_02)[3]
typeIerror_logistic_reduced_over_02 = false_positive_logistic_reduced_over_02 / negative_logistic_reduced_over_02

typeIerror_logistic_reduced_over_02

true_positive_logistic_reduced_over_02 = table(test_set$IsWinner, logistic_reduced_over_predictions_02)[4]
positive_logistic_reduced_over_02 = table(test_set$IsWinner, logistic_reduced_over_predictions_02)[2] + table(test_set$IsWinner, logistic_reduced_over_predictions_02)[4]
sensitivity_logistic_reduced_over_02 = true_positive_logistic_reduced_over_02 / positive_logistic_reduced_over_02

sensitivity_logistic_reduced_over_02
```

At this threshold, the Type 1 Error was 0.853 and the Sensitivity was 0.958, which are the same exact values for the full oversampled logistic model at this threshold.

##### Threshold = 0.3

```{r}
# Threshold = 0.3

logistic_reduced_over_predictions_03 = ifelse(logistic_reduced_over_predictions > 0.3, 1, 0)
logistic_reduced_over_accuracy_03 = sum(logistic_reduced_over_predictions_03 == test_set[2]) / dim(test_set[2])[1]

table(test_set$IsWinner, logistic_reduced_over_predictions_03)

false_positive_logistic_reduced_over_03 = table(test_set$IsWinner, logistic_reduced_over_predictions_03)[3]
negative_logistic_reduced_over_03 = table(test_set$IsWinner, logistic_reduced_over_predictions_03)[1] + table(test_set$IsWinner, logistic_reduced_over_predictions_03)[3]
typeIerror_logistic_reduced_over_03 = false_positive_logistic_reduced_over_03 / negative_logistic_reduced_over_03

typeIerror_logistic_reduced_over_03

true_positive_logistic_reduced_over_03 = table(test_set$IsWinner, logistic_reduced_over_predictions_03)[4]
positive_logistic_reduced_over_03 = table(test_set$IsWinner, logistic_reduced_over_predictions_03)[2] + table(test_set$IsWinner, logistic_reduced_over_predictions_03)[4]
sensitivity_logistic_reduced_over_03 = true_positive_logistic_reduced_over_03 / positive_logistic_reduced_over_03

sensitivity_logistic_reduced_over_03
```

With a threshold of 0.3, when compared to threshold 0.2, the model again produced a lower Type 1 Error of 0.68 and a lower Sensitivity of 0.792. These values are very similar to the full oversampled logistic model at this threshold as well.

##### Threshold = 0.4

```{r}
# Threshold = 0.4

logistic_reduced_over_predictions_04 = ifelse(logistic_reduced_over_predictions > 0.4, 1, 0)
logistic_reduced_over_accuracy_04 = sum(logistic_reduced_over_predictions_04 == test_set[2]) / dim(test_set[2])[1]

table(test_set$IsWinner, logistic_reduced_over_predictions_04)

false_positive_logistic_reduced_over_04 = table(test_set$IsWinner, logistic_reduced_over_predictions_04)[3]
negative_logistic_reduced_over_04 = table(test_set$IsWinner, logistic_reduced_over_predictions_04)[1] + table(test_set$IsWinner, logistic_reduced_over_predictions_04)[3]
typeIerror_logistic_reduced_over_04 = false_positive_logistic_reduced_over_04 / negative_logistic_reduced_over_04

typeIerror_logistic_reduced_over_04

true_positive_logistic_reduced_over_04 = table(test_set$IsWinner, logistic_reduced_over_predictions_04)[4]
positive_logistic_reduced_over_04 = table(test_set$IsWinner, logistic_reduced_over_predictions_04)[2] + table(test_set$IsWinner, logistic_reduced_over_predictions_04)[4]
sensitivity_logistic_reduced_over_04 = true_positive_logistic_reduced_over_04 / positive_logistic_reduced_over_04

sensitivity_logistic_reduced_over_04
```

Using the threshold of 0.4 again produced the same results to the full oversampled logistic model, with a Type 1 Error of 0.5 and a Sensitivity of 0.75. This suggests the models are extremely similar.

#### ROC Curve

```{r}
# ROC curve

roc.out <- roc(test_set$IsWinner, logistic_reduced_over_predictions)
plot(roc.out, print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")
auc(roc.out)
```

The ROC curve adds to the evidence that the models are very similar, producing the exact same AUC value of the full oversampled logistic model: 0.579.

### Discriminant Analysis

Discriminant analysis is a statistical technique based on the Bayes theorem used to solve classification problems.

This technique assumes that the distributions of the independent variables conditioned to the dependent variable are normal. 
In order to verify if our variables satisfy this condition we computed the Shapiro-Wilk test. The null hypothesis of this test is that a sample comes from a Normal distribution. We reject the null hypothesis if the p-value is less then 0.05, which means that the probability of that sample to have been generated from a Normal distribution is less that 5%.

```{r}
shapiro.test(danceability[IsWinner == 0]) # Yes
shapiro.test(danceability[IsWinner == 1]) # Yes

shapiro.test(acousticness[IsWinner == 0]) # No
shapiro.test(acousticness[IsWinner == 1]) # No

shapiro.test(duration_ms[IsWinner == 0]) # No
shapiro.test(duration_ms[IsWinner == 1]) # No

shapiro.test(energy[IsWinner == 0]) # No
shapiro.test(energy[IsWinner == 1]) # No

shapiro.test(instrumentalness[IsWinner == 0]) # No
shapiro.test(instrumentalness[IsWinner == 1]) # No

shapiro.test(loudness[IsWinner == 0]) # No
shapiro.test(loudness[IsWinner == 1]) # No

shapiro.test(tempo[IsWinner == 0]) # No
shapiro.test(tempo[IsWinner == 1]) # No

shapiro.test(valence[IsWinner == 0]) # No
shapiro.test(valence[IsWinner == 1]) # No
```

#### QQ-plots of the conditioned independent variables

We also used a visual tool to check if the variables follow a Normal distribution.

##### Danceability
```{r}
par(mfrow = c(1, 2))

qqnorm(danceability[IsWinner == 0], main="Normal Q-Q Non-Nominated")
grid()               
qqline(danceability[IsWinner == 0],lwd = 2, col = "red")

qqnorm(danceability[IsWinner == 1], main="Normal Q-Q Nominated")
grid()               
qqline(danceability[IsWinner == 1],lwd = 2, col = "red")
```

Danceability looks normal confirming the result of the test.

##### Acousticness
```{r}
par(mfrow = c(1, 2))

qqnorm(acousticness[IsWinner == 0], main="Normal Q-Q Non-Nominated")
grid()               
qqline(acousticness[IsWinner == 0],lwd = 2, col = "red")

qqnorm(acousticness[IsWinner == 1], main="Normal Q-Q Nominated")
grid()               
qqline(acousticness[IsWinner == 1],lwd = 2, col = "red")
```

Acousticness is heavily not Normal, following a S-shape.

##### Duration
```{r}
par(mfrow = c(1, 2))

qqnorm(duration_ms[IsWinner == 0], main="Normal Q-Q Non-Nominated")
grid()               
qqline(duration_ms[IsWinner == 0],lwd = 2, col = "red")

qqnorm(duration_ms[IsWinner == 1], main="Normal Q-Q Nominated")
grid()               
qqline(duration_ms[IsWinner == 1],lwd = 2, col = "red")
```

Duration shows a heavy right tail.

##### Energy
```{r}
par(mfrow = c(1, 2))

qqnorm(energy[IsWinner == 0], main="Normal Q-Q Non-Nominated")
grid()               
qqline(energy[IsWinner == 0],lwd = 2, col = "red")

qqnorm(energy[IsWinner == 1], main="Normal Q-Q Nominated")
grid()               
qqline(energy[IsWinner == 1],lwd = 2, col = "red")
```

The energy feature shows not normal tails.

##### Instrumentalness
```{r}
par(mfrow = c(1, 2))

qqnorm(instrumentalness[IsWinner == 0], main="Normal Q-Q Non-Nominated")
grid()               
qqline(instrumentalness[IsWinner == 0],lwd = 2, col = "red")

qqnorm(instrumentalness[IsWinner == 1], main="Normal Q-Q Nominated")
grid()               
qqline(instrumentalness[IsWinner == 1],lwd = 2, col = "red")
```

Instrumentalness shows a very heavy  right tail.

##### Loudness
```{r}
par(mfrow = c(1, 2))

qqnorm(loudness[IsWinner == 0], main="Normal Q-Q Non-Nominated")
grid()               
qqline(loudness[IsWinner == 0],lwd = 2, col = "red")

qqnorm(loudness[IsWinner == 1], main="Normal Q-Q Nominated")
grid()               
qqline(loudness[IsWinner == 1],lwd = 2, col = "red")
```

Loudness shows not normal tails.

##### Tempo
```{r}
par(mfrow = c(1, 2))

qqnorm(tempo[IsWinner == 0], main="Normal Q-Q Non-Nominated")
grid()               
qqline(tempo[IsWinner == 0],lwd = 2, col = "red")

qqnorm(tempo[IsWinner == 1], main="Normal Q-Q Nominated")
grid()               
qqline(tempo[IsWinner == 1],lwd = 2, col = "red")
```

The tempo variable seems close to a normal distribution.

##### Valence
```{r}
# valence tails slightly not normal
par(mfrow = c(1, 2))

qqnorm(valence[IsWinner == 0], main="Normal Q-Q Non-Nominated")
grid()               
qqline(valence[IsWinner == 0],lwd = 2, col = "red")

qqnorm(valence[IsWinner == 1], main="Normal Q-Q Nominated")
grid()               
qqline(valence[IsWinner == 1],lwd = 2, col = "red")
```

The valence variable seems close to a normal distribution exept for the tails.

#### Transformations

The only variable to have passed the test is Danceability, so we decided to apply a transformation to the other variables to make them more Normal-like.
We decided to use the Box-Cox transform estimating the parameter.

We then test them looking again at the Shapiro test and at the plots to check if the transformation succeeded in making the distributions more Normal-like.

##### Acousticness

```{r}
par(mfrow = c(1, 1))

b_acousticness <- boxcox(lm(acousticness ~ 1))
lambda <- b_acousticness$x[which.max(b_acousticness$y)]
acousticness_tran <- (acousticness ^ lambda - 1) / lambda

par(mfrow = c(1, 2))

qqnorm(acousticness_tran[IsWinner == 0], main="Normal Q-Q Non-Nominated")
grid()               
qqline(acousticness_tran[IsWinner == 0],lwd = 2, col = "red")

qqnorm(acousticness_tran[IsWinner == 1], main="Normal Q-Q Nominated")
grid()               
qqline(acousticness_tran[IsWinner == 1],lwd = 2, col = "red")

shapiro.test(acousticness_tran[IsWinner == 0]) # No 
shapiro.test(acousticness_tran[IsWinner == 1]) # No
```

The transformed variable didn't pass the test but the plots show a good improvement, so we will use the transformed variable.

##### Duration

```{r}
# duration_ms plots improved, test not passed

par(mfrow = c(1, 1))

b_duration_ms <- boxcox(lm(duration_ms ~ 1))
lambda <- b_duration_ms$x[which.max(b_duration_ms$y)]
duration_ms_tran <- (duration_ms ^ lambda - 1) / lambda

par(mfrow = c(1, 2))

qqnorm(duration_ms_tran[IsWinner == 0], main="Normal Q-Q Non-Nominated")
grid()               
qqline(duration_ms_tran[IsWinner == 0],lwd = 2, col = "red")

qqnorm(duration_ms_tran[IsWinner == 1], main="Normal Q-Q Nominated")
grid()               
qqline(duration_ms_tran[IsWinner == 1],lwd = 2, col = "red")

shapiro.test(duration_ms_tran[IsWinner == 0]) # No 
shapiro.test(duration_ms_tran[IsWinner == 1]) # No
```

The transformed variable didn't pass the test but the plots show an improvement in the tails, so we will use the transformed variable.

##### Energy

```{r}
par(mfrow = c(1, 1))

b_energy <- boxcox(lm(energy ~ 1))
lambda <- b_energy$x[which.max(b_energy$y)]
energy_tran <- (energy ^ lambda - 1) / lambda

par(mfrow = c(1, 2))

qqnorm(energy_tran[IsWinner == 0], main="Normal Q-Q Non-Nominated")
grid()               
qqline(energy_tran[IsWinner == 0],lwd = 2, col = "red")

qqnorm(energy_tran[IsWinner == 1], main="Normal Q-Q Nominated")
grid()               
qqline(energy_tran[IsWinner == 1],lwd = 2, col = "red")

shapiro.test(energy_tran[IsWinner == 0]) # No 
shapiro.test(energy_tran[IsWinner == 1]) # No
```

The transformed variable didn't pass the test and the plots don't show improvements so we decided to keep the original values.

##### Instrumentalness

```{r}
# instrumentalness can't apply the boxcox transformation because there are some 0's

par(mfrow = c(1, 1))

# Added a small value to overcome the issue of 0's
new_instrumentalness = instrumentalness + 1e-05

b_instrumentalness <- boxcox(lm(new_instrumentalness ~ 1))
lambda <- b_instrumentalness$x[which.max(b_instrumentalness$y)]
instrumentalness_tran <- (new_instrumentalness ^ lambda - 1) / lambda

par(mfrow = c(1, 2))

qqnorm(instrumentalness_tran[IsWinner == 0], main="Normal Q-Q Non-Nominated")
grid()               
qqline(instrumentalness_tran[IsWinner == 0],lwd = 2, col = "red")

qqnorm(instrumentalness_tran[IsWinner == 1], main="Normal Q-Q Nominated")
grid()               
qqline(instrumentalness_tran[IsWinner == 1],lwd = 2, col = "red")

shapiro.test(instrumentalness_tran[IsWinner == 0]) # No 
shapiro.test(instrumentalness_tran[IsWinner == 1]) # No
```

We could not directly apply the boxcox transformation to the instrumentalness variable because of the presence of zeros, so we added a small value and then applied it. The new variable did not pass the test and the plots did not improve, so we decided to keep the original data.

##### Loudness

```{r}
par(mfrow = c(1, 1))

new_loudness = loudness * (-1)

b_loudness <- boxcox(lm(new_loudness ~ 1))
lambda <- b_loudness$x[which.max(b_loudness$y)]
loudness_tran <- (new_loudness ^ lambda - 1) / lambda

par(mfrow = c(1, 2))

qqnorm(loudness_tran[IsWinner == 0], main="Normal Q-Q Non-Nominated")
grid()               
qqline(loudness_tran[IsWinner == 0],lwd = 2, col = "red")

qqnorm(loudness_tran[IsWinner == 1], main="Normal Q-Q Nominated")
grid()               
qqline(loudness_tran[IsWinner == 1],lwd = 2, col = "red")

shapiro.test(loudness_tran[IsWinner == 0]) # Yes
shapiro.test(loudness_tran[IsWinner == 1]) # Yes
```

We could not directly apply the boxcox transformation to the variable loudness because the variable takes negative values. We multiplied the values by -1 and then applied the transformation. The new variable passed the test and the plots confirm the result of the test. Since we have multiplied the variable by -1 the interpretation of the variable will be mirrored.

##### Tempo

```{r}
# tempo, slight improvement in the plots

par(mfrow = c(1, 1))

b_tempo <- boxcox(lm(tempo ~ 1))
lambda <- b_tempo$x[which.max(b_tempo$y)]
tempo_tran <- (tempo ^ lambda - 1) / lambda

par(mfrow = c(1, 2))

qqnorm(tempo_tran[IsWinner == 0], main="Normal Q-Q Non-Nominated")
grid()               
qqline(tempo_tran[IsWinner == 0],lwd = 2, col = "red")

qqnorm(tempo_tran[IsWinner == 1], main="Normal Q-Q Nominated")
grid()               
qqline(tempo_tran[IsWinner == 1],lwd = 2, col = "red")

shapiro.test(tempo_tran[IsWinner == 0]) # No
shapiro.test(tempo_tran[IsWinner == 1]) # No
```

The transformed tempo variable showed slight improvement in the plots but still didn't pass the test, we used the transformed variable.

##### Valence

```{r}
# valence, pretty much the same

par(mfrow = c(1, 1))

b_valence <- boxcox(lm(valence ~ 1))
lambda <- b_valence$x[which.max(b_valence$y)]
valence_tran <- (valence ^ lambda - 1) / lambda

par(mfrow = c(1, 2))

qqnorm(valence_tran[IsWinner == 0], main="Normal Q-Q Non-Nominated")
grid()               
qqline(valence_tran[IsWinner == 0],lwd = 2, col = "red")

qqnorm(valence_tran[IsWinner == 1], main="Normal Q-Q Nominated")
grid()               
qqline(valence_tran[IsWinner == 1],lwd = 2, col = "red")

shapiro.test(valence_tran[IsWinner == 0]) # No
shapiro.test(valence_tran[IsWinner == 1]) # No
```

The transformed variable didn't change much so we kept the original data.

(SHOULD WE COMPARE A FEW OF THE QQ PLOTS BEFORE AND AFTER THE TRANSFORMATION SIDE-BY-SIDE AS A GOOD VISUAL? FOR THE FEATURES THAT CHANGED AND FOR ONE THAT DID NOT?)

### Data transformation

```{r}
par(mfrow = c(1, 1))

b_data_acousticness <- boxcox(lm(data$acousticness ~ 1))
lambda <- b_data_acousticness$x[which.max(b_data_acousticness$y)]
data_acousticness_tran <- (data$acousticness ^ lambda - 1) / lambda

b_data_duration_ms <- boxcox(lm(data$duration_ms ~ 1))
lambda <- b_data_duration_ms$x[which.max(b_data_duration_ms$y)]
data_duration_ms_tran <- (data$duration_ms ^ lambda - 1) / lambda

neg_loudness = data$loudness * (-1)
b_data_loudness <- boxcox(lm(neg_loudness ~ 1))
lambda <- b_data_loudness$x[which.max(b_data_loudness$y)]
data_loudness_tran <- (neg_loudness ^ lambda - 1) / lambda

b_data_tempo <- boxcox(lm(data$tempo ~ 1))
lambda <- b_data_tempo$x[which.max(b_data_tempo$y)]
data_tempo_tran <- (data$tempo ^ lambda - 1) / lambda

tran_data = matrix(c(data$IsWinner, data_acousticness_tran,
                     data_duration_ms_tran, 
                     data$energy, data$instrumentalness, data_loudness_tran,
                     data_tempo_tran, data$valence), ncol = 8)

training_tran_data = tran_data[train_ind,]

test_tran_data = tran_data[-train_ind,]

colnames_tran_data = c("IsWinner", "acousticness_tran", "duration_ms_tran",
                       "energy","instrumentalness", "loudness_tran",
                       "tempo_tran", "valence")

colnames(training_tran_data) = colnames_tran_data
colnames(test_tran_data) = colnames_tran_data
colnames(tran_data) = colnames_tran_data


training_tran_data = as.data.frame(training_tran_data)
test_tran_data = as.data.frame(test_tran_data)
tran_data = as.data.frame(tran_data)
```


### Linear Discriminant Analysis

```{r}
## LDA

simple_lda = lda(IsWinner ~ acousticness_tran + duration_ms_tran + 
                energy + instrumentalness + loudness_tran + tempo_tran +
                valence, data = training_tran_data, family = "binomial")

pred_simple_lda = predict(simple_lda, newdata = test_tran_data,
                          type = "Response")

# ROC curve

roc.out <- roc(test_set$IsWinner, as.numeric(pred_simple_lda$class) - 1)
plot(roc.out, print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate",
     ylab="True positive rate")
auc(roc.out)
```

The ROC curve tells us that the predictions computed with this method are worse than a coin toss, so we try to fit the model to the oversampled dataset.

### Oversampled LDA 

```{r}
# Oversampled LDA

oversampled_train_tran_data = ovun.sample(IsWinner ~., data = training_tran_data, method = "over", p = 0.5, seed = 42)$data

simple_over_lda = lda(IsWinner ~ acousticness_tran + duration_ms_tran + 
                   energy + instrumentalness + loudness_tran + tempo_tran +
                   valence, data = oversampled_train_tran_data,
                   family = "binomial")


pred_simple_over_lda = predict(simple_over_lda, newdata = test_tran_data,
                               type = "Response")

# ROC curve

roc.out <- roc(test_set$IsWinner, as.numeric(pred_simple_over_lda$class) - 1)
plot(roc.out, print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate",
     ylab="True positive rate")
auc(roc.out)
```

The ROC curve shows a slight improvement, but the value is just above 0.5. 

#### Predictions

Making predictions on the oversampled LDA

##### Threshold = 0.4
```{r}

lda_over_predictions_04 = ifelse(pred_simple_over_lda$posterior[,1] > 0.4, 1, 0)
lda_over_accuracy_04 = sum(lda_over_predictions_04 == test_set[2]) / dim(test_set[2])[1]

table(test_set$IsWinner, lda_over_predictions_04)

false_positive_lda_over_04 = table(test_set$IsWinner, lda_over_predictions_04)[3]
negative_lda_over_04 = table(test_set$IsWinner, lda_over_predictions_04)[1] + table(test_set$IsWinner, lda_over_predictions_04)[3]
typeIerror_lda_over_04 = false_positive_lda_over_04 / negative_lda_over_04
typeIerror_lda_over_04

true_positive_lda_over_04 = table(test_set$IsWinner, lda_over_predictions_04)[4]
positive_lda_over_04 = table(test_set$IsWinner, lda_over_predictions_04)[2] + table(test_set$IsWinner, lda_over_predictions_04)[4]
sensitivity_lda_over_04 = true_positive_lda_over_04 / positive_lda_over_04
sensitivity_lda_over_04
```

##### Threshold = 0.5

```{r}
lda_over_predictions_05 = list(pred_simple_over_lda$class)
lda_over_accuracy_05 = sum(lda_over_predictions_05 == test_set[2]) / dim(test_set[2])[1]

table(test_set$IsWinner, lda_over_predictions_05[[1]])

false_positive_lda_over_05 = table(test_set$IsWinner, lda_over_predictions_05[[1]])[3]
negative_lda_over_05 = table(test_set$IsWinner, lda_over_predictions_05[[1]])[1] + table(test_set$IsWinner, lda_over_predictions_05[[1]])[3]
typeIerror_lda_over_05 = false_positive_lda_over_05 / negative_lda_over_05
typeIerror_lda_over_05

true_positive_lda_over_05 = table(test_set$IsWinner, lda_over_predictions_05[[1]])[4]
positive_lda_over_05 = table(test_set$IsWinner, lda_over_predictions_05[[1]])[2] + table(test_set$IsWinner, lda_over_predictions_05[[1]])[4]
sensitivity_lda_over_05 = true_positive_lda_over_05 / positive_lda_over_05
sensitivity_lda_over_05

```

Setting the threshold to 0.4 leads to high type I error and sensitivity,
this means that the model is able to detect the nominated song but classifies many not nominated songs as nominated. While setting the threshold to 0.5 leads to low type I error and sensitivity, which means that many nominated songs are classified as not nominated, but most not nominated songs are correctly classified. 

#### QDA

```{r}
## QDA

qda = qda(IsWinner ~ acousticness_tran + duration_ms_tran + 
            energy + instrumentalness + loudness_tran + tempo_tran +
            valence, data = training_tran_data, family = "binomial")

pred_qda = predict(qda, newdata = test_tran_data, type = "Response")

# ROC

roc.out <- roc(test_set$IsWinner, as.numeric(pred_qda$class) - 1)
plot(roc.out, print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate",
     ylab="True positive rate")
auc(roc.out)
```

The ROC curve shows a slight improvement compared to the linear discriminant analysis, but the area under the curve is still just above 0.5, so we try to fit the model to the oversampled dataset.

```{r}
# QDA oversampled

qda_over = qda(IsWinner ~ acousticness_tran + duration_ms_tran + 
            energy + instrumentalness + loudness_tran + tempo_tran +
            valence, data = oversampled_train_tran_data, family = "binomial")

pred_qda_over = predict(qda_over, newdata = test_tran_data, type = "Response")

# ROC

roc.out <- roc(test_set$IsWinner, as.numeric(pred_qda_over$class) - 1)
plot(roc.out, print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate",
     ylab="True positive rate")
auc(roc.out)
```

The ROC curve shows a significant improvement.

#### Predictions

##### Threshold = 0.4

```{r}
# Predictions

# Threshold 0.4

qda_over_predictions_04 = ifelse(pred_qda_over$posterior[,1] > 0.4, 1, 0)
qda_over_accuracy_04 = sum(qda_over_predictions_04 == test_set[2]) / dim(test_set[2])[1]

table(test_set$IsWinner, qda_over_predictions_04)

false_positive_qda_over_04 = table(test_set$IsWinner, qda_over_predictions_04)[3]
negative_qda_over_04 = table(test_set$IsWinner, qda_over_predictions_04)[1] + table(test_set$IsWinner, qda_over_predictions_04)[3]
typeIerror_qda_over_04 = false_positive_qda_over_04 / negative_qda_over_04
typeIerror_qda_over_04

true_positive_qda_over_04 = table(test_set$IsWinner, qda_over_predictions_04)[4]
positive_qda_over_04 = table(test_set$IsWinner, qda_over_predictions_04)[2] + table(test_set$IsWinner, qda_over_predictions_04)[4]
sensitivity_qda_over_04 = true_positive_qda_over_04 / positive_qda_over_04
sensitivity_qda_over_04
```

##### Threshold = 0.5

```{r}
# Threshold 0.5

qda_over_predictions_05 = list(pred_qda_over$class)
qda_over_accuracy_05 = sum(qda_over_predictions_05 == test_set[2]) / dim(test_set[2])[1]

table(test_set$IsWinner, qda_over_predictions_05[[1]])

false_positive_qda_over_05 = table(test_set$IsWinner, qda_over_predictions_05[[1]])[3]
negative_qda_over_05 = table(test_set$IsWinner, qda_over_predictions_05[[1]])[1] + table(test_set$IsWinner, qda_over_predictions_05[[1]])[3]
typeIerror_qda_over_05 = false_positive_qda_over_05 / negative_qda_over_05
typeIerror_qda_over_05

true_positive_qda_over_05 = table(test_set$IsWinner, qda_over_predictions_05[[1]])[4]
positive_qda_over_05 = table(test_set$IsWinner, qda_over_predictions_05[[1]])[2] + table(test_set$IsWinner, qda_over_predictions_05[[1]])[4]
sensitivity_qda_over_05 = true_positive_qda_over_05 / positive_qda_over_05
sensitivity_qda_over_05
```

Setting the threshold to 0.5 gave better results both in terms of type I error and sensitivity, in particular the model classifies as not nominated 50% of the songs that weren't nominated and classifies 70% of the song that were nominated as nominated.

## Regularized Regression

Regularized regression models are a type of regression with an additional constraint on the parameter designed to prevent overfitting and deal with multicollinearity.

### Ridge Regression

Ridge regression exploits L2 regularization to shrink the estimated parameters towards zero.

```{r}
## Ridge regression

ridge_cv = cv.glmnet(data.matrix(oversampled_train_data[,-1]), oversampled_train_data$IsWinner, alpha = 0, family = "binomial",
                     type.measure = "class")
plot(ridge_cv)

lambda_opt_ridge = ridge_cv$lambda.min

pred_ridge = predict(ridge_cv, data.matrix(test_set[, c(-1, -2)]), type = "class", s = lambda_opt_ridge)

table(test_set$IsWinner, pred_ridge)

false_positive_ridge = table(test_set$IsWinner, pred_ridge)[3]
negative_ridge = table(test_set$IsWinner, pred_ridge)[1] + table(test_set$IsWinner, pred_ridge)[3]
typeIerror_ridge = false_positive_ridge / negative_ridge
typeIerror_ridge

true_positive_ridge = table(test_set$IsWinner, pred_ridge)[4]
positive_ridge = table(test_set$IsWinner, pred_ridge)[2] + table(test_set$IsWinner, pred_ridge)[4]
sensitivity_ridge = true_positive_ridge / positive_ridge
sensitivity_ridge
```

Ridge regression predictions showed a low type I error and sensitivity, meaning that the model is able to identify not nominated songs, but struggles to correctly classify nominated songs.

### Lasso Regression

Lasso regression uses L1 regularization to force some of the estimated parameter to be exactly zero.

```{r}
## Lasso regression

lasso_cv = cv.glmnet(data.matrix(oversampled_train_data[,-1]), oversampled_train_data$IsWinner,
                     alpha = 1, family = "binomial", type.measure = "class")

plot(lasso_cv)

lambda_opt_lasso = lasso_cv$lambda.min

pred_lasso = predict(lasso_cv, data.matrix(test_set[, c(-1, -2)]), type = "class", s = lambda_opt_ridge)

table(test_set$IsWinner, pred_lasso)

false_positive_lasso = table(test_set$IsWinner, pred_lasso)[3]
negative_lasso = table(test_set$IsWinner, pred_lasso)[1] + table(test_set$IsWinner, pred_lasso)[3]
typeIerror_lasso = false_positive_lasso / negative_lasso
typeIerror_lasso

true_positive_lasso = table(test_set$IsWinner, pred_lasso)[4]
positive_lasso = table(test_set$IsWinner, pred_lasso)[2] + table(test_set$IsWinner, pred_lasso)[4]
sensitivity_lasso = true_positive_lasso / positive_lasso
sensitivity_lasso
```

Lasso regression gave similar results compared to ridge regression in terms of type I error and slightly worse results in terms of sensitivity, therefore the same comments apply.

## KNN

KNN (K-Nearest Neighbors) models are non-parametric and offer a flexible decision boundary for classification problems. The model will classify a song based on the "k" nearest neighbors of said song in the parameter space.

```{r}
# K-NN

min_max_norm = function(x) {
  (x - min(x)) / (max(x) - min(x))
}

normalized_data = as.data.frame(lapply(data[,c(-1, -2, -9, -11, -13)], min_max_norm))

IsWinner_norm = data$IsWinner

normalized_data = cbind(IsWinner_norm, normalized_data)

training_norm_data = normalized_data[train_ind,]

test_norm_data = normalized_data[-train_ind,]
```

After normalizing the data, the selection of the previously mentioned "k" value is performed to determine the most efficient number of "neighbors" the model will consider when classifying a song.

```{r}
# Selecting k

kmax = 100

test_error = numeric(kmax)

for (k in 1:kmax) {
  knn_pred = as.factor(knn(training_norm_data[,-1], test_norm_data[,-1],
                            cl = training_norm_data$IsWinner_norm, k = k))
  cm = confusionMatrix(data = knn_pred,
                       reference = as.factor(test_norm_data$IsWinner_norm))
  test_error[k] = 1 - cm$overall[1]
}
k_min = which.min(test_error)
k_min
```

After analyzing the original training data set, the KNN model determined 25 to be the most optimal number of neighbors to consider.

```{r}
knn = knn(training_norm_data[,-1], test_norm_data[,-1],
          cl = training_norm_data$IsWinner_norm, k = k_min)

knn_pred_min = as.factor(knn)

table(test_norm_data$IsWinner_norm, knn)
```

Utilizing the chosen K value, the model yields an accuracy of 86.8% and predicts all of the non-nominated songs correctly, but only one of the nominated songs correctly. This points towards a heavy bias classifying songs as non-nominated.

### Oversampling

Now the same process as above is repeated, but by building a model using the oversampled data.

```{r}
# oversampled

test_over_error = numeric(kmax)

normalized_over_data = as.data.frame(lapply(oversampled_train_data[,c(-8, -10, -12)], min_max_norm))

training_norm_data_over = normalized_over_data[train_ind,]

for (k in 1:kmax) {
  knn_over_pred = as.factor(knn(training_norm_data_over[,-1],
                                test_norm_data[,-1],
                                cl = training_norm_data$IsWinner_norm, k = k))
  cm_over = confusionMatrix(data = knn_over_pred,
                            reference = as.factor(test_norm_data$IsWinner_norm))
  test_over_error[k] = 1 - cm_over$overall[1]
}


k_min_over = which.min(test_over_error)
k_min_over
```

The chosen value of "k" for the oversampled dataset is 14.

```{r}
knn_over = knn(training_norm_data_over[,-1], test_norm_data[,-1],
          cl = training_norm_data$IsWinner_norm, k = k_min_over)

knn_pred_min_over = as.factor(knn_over)

table(test_norm_data$IsWinner_norm, knn_over)
```

Utilizing the chosen "k" value, the KNN model yields a classification accuracy of 84.5%, but again only one of the nominated songs were correctly classified.

# Results and Conclusions

In the end...

SHOW TABLES, GRAPHS, CHARTS, HIGHLIGHT THE MOST IMPORTANT INFORMATION ONLY. DO NOT INUNDATE THIS SECTION.

Future work could expand upon our research by...

- Change the time_signature to be binary; 4 or not_4
- the addition of new variables for the songs
- Investigating different types of models
- different accuracy metrics during the hyperparameter tuning of KNN and/or early stopping